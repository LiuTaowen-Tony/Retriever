RerieverConfig_small()
	batch_size: 32
	beta1: 0.9
	beta2: 0.95
	grad_clip: 1.0
	gradient_accumulation_steps: 4
	hidden_size: 512
	learning_rate: 0.0006
	lr_decay_iters: 80000
	max_iters: 80000
	min_lr: 6e-05
	num_heads: 8
	num_layers: 6
	sequence_length: 1024
	vocab_size: 32000
	warmup_iters: 2000
	weight_decay: 0.1

OptimizedModule(
  (_orig_mod): Retriever(
    (token_embedding): Embedding(32000, 512)
    (layers): ModuleList(
      (0-5): 6 x DecoderLayer(
        (ln_1): RMSNorm()
        (attn): Attention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (ln_2): RMSNorm()
        (mlp): MLP(
          (gate_proj): Linear(in_features=512, out_features=1372, bias=False)
          (up_proj): Linear(in_features=512, out_features=1372, bias=False)
          (down_proj): Linear(in_features=1372, out_features=512, bias=False)
        )
      )
    )
    (norm): RMSNorm()
    (lm_head): Linear(in_features=512, out_features=32000, bias=False)
  )
)

num decayed parameter tensors: 43, with 35,319,808 parameters
num non-decayed parameter tensors: 13, with 6,656 parameters
number of total parameters: 35.33M
all train file nums: 1024
preparing first dataset file english_c4/c4-train.00000-of-01024.txt
finished, consume: 121s
processing 0: english_c4/c4-train.00000-of-01024.txt, origin: 355732, samples: 173721, accum_tokens: 177M, iter_num: 0
step 100, loss 9.63, lr 0.000030, consume 109.80s
step 200, loss 7.81, lr 0.000060, consume 97.50s
step 300, loss 6.67, lr 0.000090, consume 74.37s
step 400, loss 6.17, lr 0.000120, consume 74.50s
step 500, loss 5.90, lr 0.000150, consume 74.52s
step 600, loss 5.69, lr 0.000180, consume 74.53s
step 700, loss 5.51, lr 0.000210, consume 74.52s
step 800, loss 5.36, lr 0.000240, consume 74.52s
step 900, loss 5.22, lr 0.000270, consume 74.54s
step 1000, loss 5.09, lr 0.000300, consume 74.51s
step 1100, loss 4.98, lr 0.000330, consume 74.52s
step 1200, loss 4.89, lr 0.000360, consume 74.51s
step 1300, loss 4.80, lr 0.000390, consume 74.52s
processing 1: english_c4/c4-train.00001-of-01024.txt, origin: 355708, samples: 173229, accum_tokens: 355M, iter_num: 1357
step 1400, loss 4.71, lr 0.000420, consume 95.08s
step 1500, loss 4.61, lr 0.000450, consume 76.55s
step 1600, loss 4.52, lr 0.000480, consume 95.68s
step 1700, loss 4.43, lr 0.000510, consume 74.51s
step 1800, loss 4.36, lr 0.000540, consume 74.54s
step 1900, loss 4.32, lr 0.000570, consume 74.54s
step 2000, loss 4.26, lr 0.000600, consume 74.56s
step 2100, loss 4.22, lr 0.000600, consume 74.54s
step 2200, loss 4.19, lr 0.000600, consume 74.55s
step 2300, loss 4.15, lr 0.000600, consume 74.37s
step 2400, loss 4.12, lr 0.000600, consume 74.34s
step 2500, loss 4.09, lr 0.000600, consume 74.33s
step 2600, loss 4.06, lr 0.000600, consume 74.74s
step 2700, loss 4.04, lr 0.000600, consume 74.33s
processing 2: english_c4/c4-train.00002-of-01024.txt, origin: 355718, samples: 173903, accum_tokens: 533M, iter_num: 2710
step 2800, loss 4.03, lr 0.000600, consume 94.53s
step 2900, loss 4.01, lr 0.000600, consume 96.65s
step 3000, loss 4.00, lr 0.000600, consume 74.20s
step 3100, loss 3.99, lr 0.000600, consume 74.36s
step 3200, loss 3.97, lr 0.000600, consume 74.37s
step 3300, loss 3.95, lr 0.000600, consume 74.38s
step 3400, loss 3.94, lr 0.000600, consume 74.36s
step 3500, loss 3.92, lr 0.000600, consume 74.34s
step 3600, loss 3.91, lr 0.000599, consume 74.36s
step 3700, loss 3.90, lr 0.000599, consume 74.35s
step 3800, loss 3.89, lr 0.000599, consume 74.35s
step 3900, loss 3.89, lr 0.000599, consume 74.35s
step 4000, loss 3.88, lr 0.000599, consume 74.36s
processing 3: english_c4/c4-train.00003-of-01024.txt, origin: 355753, samples: 173350, accum_tokens: 710M, iter_num: 4068
step 4100, loss 3.87, lr 0.000599, consume 93.76s
step 4200, loss 3.87, lr 0.000599, consume 76.19s
step 4300, loss 3.85, lr 0.000599, consume 95.24s
step 4400, loss 3.84, lr 0.000599, consume 74.33s
step 4500, loss 3.84, lr 0.000599, consume 74.38s
step 4600, loss 3.83, lr 0.000599, consume 74.39s
step 4700, loss 3.81, lr 0.000598, consume 74.39s
step 4800, loss 3.82, lr 0.000598, consume 74.37s
step 4900, loss 3.80, lr 0.000598, consume 74.37s
step 5000, loss 3.81, lr 0.000598, consume 74.35s
step 5100, loss 3.80, lr 0.000598, consume 74.34s
step 5200, loss 3.79, lr 0.000598, consume 74.36s
step 5300, loss 3.79, lr 0.000598, consume 74.35s
step 5400, loss 3.78, lr 0.000597, consume 74.34s
processing 4: english_c4/c4-train.00004-of-01024.txt, origin: 355691, samples: 173171, accum_tokens: 888M, iter_num: 5423
step 5500, loss 3.78, lr 0.000597, consume 94.62s
step 5600, loss 3.78, lr 0.000597, consume 76.75s
step 5700, loss 3.77, lr 0.000597, consume 93.71s
step 5800, loss 3.77, lr 0.000597, consume 74.33s
step 5900, loss 3.76, lr 0.000597, consume 74.37s
step 6000, loss 3.76, lr 0.000597, consume 74.35s
step 6100, loss 3.75, lr 0.000596, consume 74.34s
step 6200, loss 3.75, lr 0.000596, consume 74.35s
step 6300, loss 3.74, lr 0.000596, consume 74.34s
step 6400, loss 3.74, lr 0.000596, consume 74.32s
step 6500, loss 3.73, lr 0.000596, consume 74.33s
step 6600, loss 3.74, lr 0.000595, consume 74.34s
step 6700, loss 3.73, lr 0.000595, consume 74.33s
processing 5: english_c4/c4-train.00005-of-01024.txt, origin: 355717, samples: 172927, accum_tokens: 1065M, iter_num: 6775
step 6800, loss 3.73, lr 0.000595, consume 94.59s
step 6900, loss 3.73, lr 0.000595, consume 76.03s
step 7000, loss 3.72, lr 0.000595, consume 95.13s
step 7100, loss 3.72, lr 0.000594, consume 74.28s
step 7200, loss 3.71, lr 0.000594, consume 74.35s
step 7300, loss 3.71, lr 0.000594, consume 74.37s
step 7400, loss 3.71, lr 0.000594, consume 74.38s
step 7500, loss 3.71, lr 0.000593, consume 74.34s
step 7600, loss 3.71, lr 0.000593, consume 74.36s
step 7700, loss 3.71, lr 0.000593, consume 74.34s
step 7800, loss 3.70, lr 0.000593, consume 74.32s
step 7900, loss 3.70, lr 0.000592, consume 74.33s
step 8000, loss 3.69, lr 0.000592, consume 74.35s
step 8100, loss 3.69, lr 0.000592, consume 74.35s
processing 6: english_c4/c4-train.00006-of-01024.txt, origin: 355747, samples: 173252, accum_tokens: 1242M, iter_num: 8126
step 8200, loss 3.69, lr 0.000592, consume 95.91s
step 8300, loss 3.69, lr 0.000591, consume 76.55s
step 8400, loss 3.68, lr 0.000591, consume 93.61s
step 8500, loss 3.69, lr 0.000591, consume 74.33s
step 8600, loss 3.67, lr 0.000591, consume 74.36s
step 8700, loss 3.67, lr 0.000590, consume 74.39s
step 8800, loss 3.67, lr 0.000590, consume 74.40s
step 8900, loss 3.66, lr 0.000590, consume 74.37s
step 9000, loss 3.67, lr 0.000589, consume 74.35s
step 9100, loss 3.66, lr 0.000589, consume 74.34s
step 9200, loss 3.66, lr 0.000589, consume 74.34s
step 9300, loss 3.66, lr 0.000588, consume 74.32s
step 9400, loss 3.66, lr 0.000588, consume 74.33s
processing 7: english_c4/c4-train.00007-of-01024.txt, origin: 355719, samples: 173288, accum_tokens: 1420M, iter_num: 9480
step 9500, loss 3.66, lr 0.000588, consume 93.62s
step 9600, loss 3.66, lr 0.000587, consume 76.11s
step 9700, loss 3.66, lr 0.000587, consume 94.51s
step 9800, loss 3.65, lr 0.000587, consume 74.27s
step 9900, loss 3.65, lr 0.000586, consume 74.36s
step 10000, loss 3.65, lr 0.000586, consume 74.38s
step 10100, loss 3.65, lr 0.000586, consume 74.35s
step 10200, loss 3.65, lr 0.000585, consume 74.35s
step 10300, loss 3.65, lr 0.000585, consume 74.33s
step 10400, loss 3.65, lr 0.000585, consume 74.33s
step 10500, loss 3.64, lr 0.000584, consume 74.34s
step 10600, loss 3.64, lr 0.000584, consume 74.33s
step 10700, loss 3.64, lr 0.000584, consume 74.34s
step 10800, loss 3.64, lr 0.000583, consume 74.35s
processing 8: english_c4/c4-train.00008-of-01024.txt, origin: 355746, samples: 173024, accum_tokens: 1597M, iter_num: 10833
step 10900, loss 3.64, lr 0.000583, consume 94.51s
step 11000, loss 3.64, lr 0.000582, consume 76.41s
step 11100, loss 3.64, lr 0.000582, consume 93.59s
step 11200, loss 3.64, lr 0.000582, consume 74.45s
step 11300, loss 3.63, lr 0.000581, consume 74.37s
step 11400, loss 3.64, lr 0.000581, consume 74.39s
step 11500, loss 3.63, lr 0.000580, consume 74.38s
step 11600, loss 3.63, lr 0.000580, consume 74.37s
step 11700, loss 3.62, lr 0.000580, consume 74.37s
step 11800, loss 3.62, lr 0.000579, consume 74.38s
step 11900, loss 3.62, lr 0.000579, consume 74.36s
step 12000, loss 3.63, lr 0.000578, consume 74.35s
step 12100, loss 3.62, lr 0.000578, consume 74.33s
processing 9: english_c4/c4-train.00009-of-01024.txt, origin: 355708, samples: 172984, accum_tokens: 1774M, iter_num: 12185
step 12200, loss 3.61, lr 0.000578, consume 93.52s
step 12300, loss 3.62, lr 0.000577, consume 76.61s
step 12400, loss 3.62, lr 0.000577, consume 95.51s
step 12500, loss 3.62, lr 0.000576, consume 74.28s
step 12600, loss 3.61, lr 0.000576, consume 74.38s
step 12700, loss 3.61, lr 0.000575, consume 74.38s
step 12800, loss 3.61, lr 0.000575, consume 74.38s
step 12900, loss 3.62, lr 0.000574, consume 74.39s
step 13000, loss 3.60, lr 0.000574, consume 74.35s
step 13100, loss 3.61, lr 0.000573, consume 74.34s
step 13200, loss 3.61, lr 0.000573, consume 74.35s
step 13300, loss 3.61, lr 0.000573, consume 74.35s
step 13400, loss 3.60, lr 0.000572, consume 74.36s
step 13500, loss 3.60, lr 0.000572, consume 74.35s
processing 10: english_c4/c4-train.00010-of-01024.txt, origin: 355702, samples: 173710, accum_tokens: 1952M, iter_num: 13536
step 13600, loss 3.61, lr 0.000571, consume 95.26s
step 13700, loss 3.61, lr 0.000571, consume 76.58s
step 13800, loss 3.61, lr 0.000570, consume 93.35s
step 13900, loss 3.60, lr 0.000570, consume 74.33s
step 14000, loss 3.60, lr 0.000569, consume 74.37s
step 14100, loss 3.59, lr 0.000569, consume 74.36s
step 14200, loss 3.60, lr 0.000568, consume 74.38s
step 14300, loss 3.58, lr 0.000568, consume 74.38s
step 14400, loss 3.60, lr 0.000567, consume 74.39s
step 14500, loss 3.60, lr 0.000567, consume 74.39s
step 14600, loss 3.60, lr 0.000566, consume 74.38s
step 14700, loss 3.59, lr 0.000565, consume 74.37s
step 14800, loss 3.59, lr 0.000565, consume 74.37s
processing 11: english_c4/c4-train.00011-of-01024.txt, origin: 355701, samples: 173885, accum_tokens: 2130M, iter_num: 14893
step 14900, loss 3.58, lr 0.000564, consume 93.56s
step 15000, loss 3.59, lr 0.000564, consume 76.13s
step 15100, loss 3.59, lr 0.000563, consume 95.12s
step 15200, loss 3.59, lr 0.000563, consume 74.24s
step 15300, loss 3.59, lr 0.000562, consume 74.36s
step 15400, loss 3.59, lr 0.000562, consume 74.39s
step 15500, loss 3.59, lr 0.000561, consume 74.38s
step 15600, loss 3.58, lr 0.000561, consume 74.36s
step 15700, loss 3.60, lr 0.000560, consume 74.36s
step 15800, loss 3.59, lr 0.000559, consume 74.38s
step 15900, loss 3.59, lr 0.000559, consume 74.36s
step 16000, loss 3.59, lr 0.000558, consume 74.35s
step 16100, loss 3.59, lr 0.000558, consume 74.36s
step 16200, loss 3.58, lr 0.000557, consume 74.36s
processing 12: english_c4/c4-train.00012-of-01024.txt, origin: 355710, samples: 174423, accum_tokens: 2308M, iter_num: 16252
step 16300, loss 3.58, lr 0.000556, consume 94.02s
step 16400, loss 3.58, lr 0.000556, consume 77.04s
step 16500, loss 3.58, lr 0.000555, consume 94.78s
step 16600, loss 3.58, lr 0.000555, consume 74.58s
step 16700, loss 3.58, lr 0.000554, consume 74.63s
step 16800, loss 3.58, lr 0.000553, consume 74.61s
step 16900, loss 3.58, lr 0.000553, consume 74.61s
step 17000, loss 3.57, lr 0.000552, consume 74.65s
step 17100, loss 3.58, lr 0.000552, consume 74.96s
step 17200, loss 3.57, lr 0.000551, consume 74.35s
step 17300, loss 3.57, lr 0.000550, consume 74.37s
step 17400, loss 3.57, lr 0.000550, consume 74.39s
step 17500, loss 3.57, lr 0.000549, consume 74.38s
step 17600, loss 3.57, lr 0.000548, consume 74.61s
processing 13: english_c4/c4-train.00013-of-01024.txt, origin: 355723, samples: 173966, accum_tokens: 2487M, iter_num: 17614
step 17700, loss 3.58, lr 0.000548, consume 94.77s
step 17800, loss 3.57, lr 0.000547, consume 76.64s
step 17900, loss 3.57, lr 0.000547, consume 93.53s
step 18000, loss 3.56, lr 0.000546, consume 74.37s
step 18100, loss 3.57, lr 0.000545, consume 74.39s
step 18200, loss 3.57, lr 0.000545, consume 74.42s
step 18300, loss 3.56, lr 0.000544, consume 74.39s
step 18400, loss 3.55, lr 0.000543, consume 74.39s
step 18500, loss 3.56, lr 0.000543, consume 74.38s
step 18600, loss 3.56, lr 0.000542, consume 74.39s
step 18700, loss 3.55, lr 0.000541, consume 74.69s
step 18800, loss 3.56, lr 0.000541, consume 74.38s
step 18900, loss 3.56, lr 0.000540, consume 74.39s
processing 14: english_c4/c4-train.00014-of-01024.txt, origin: 355708, samples: 174115, accum_tokens: 2665M, iter_num: 18973
step 19000, loss 3.56, lr 0.000539, consume 95.47s
step 19100, loss 3.56, lr 0.000538, consume 76.43s
step 19200, loss 3.56, lr 0.000538, consume 95.52s
step 19300, loss 3.56, lr 0.000537, consume 74.45s
step 19400, loss 3.56, lr 0.000536, consume 74.60s
step 19500, loss 3.56, lr 0.000536, consume 74.42s
step 19600, loss 3.56, lr 0.000535, consume 74.40s
step 19700, loss 3.55, lr 0.000534, consume 74.36s
step 19800, loss 3.55, lr 0.000534, consume 74.35s
step 19900, loss 3.55, lr 0.000533, consume 74.53s
step 20000, loss 3.55, lr 0.000532, consume 74.39s
step 20100, loss 3.55, lr 0.000531, consume 74.37s
step 20200, loss 3.55, lr 0.000531, consume 74.37s
step 20300, loss 3.55, lr 0.000530, consume 74.36s
processing 15: english_c4/c4-train.00015-of-01024.txt, origin: 355698, samples: 173319, accum_tokens: 2842M, iter_num: 20333
step 20400, loss 3.56, lr 0.000529, consume 97.65s
step 20500, loss 3.55, lr 0.000528, consume 76.62s
step 20600, loss 3.55, lr 0.000528, consume 93.75s
step 20700, loss 3.55, lr 0.000527, consume 74.32s
step 20800, loss 3.55, lr 0.000526, consume 74.36s
step 20900, loss 3.55, lr 0.000525, consume 74.37s
step 21000, loss 3.55, lr 0.000525, consume 74.35s
step 21100, loss 3.55, lr 0.000524, consume 74.33s
step 21200, loss 3.55, lr 0.000523, consume 74.34s
step 21300, loss 3.54, lr 0.000522, consume 74.35s
step 21400, loss 3.55, lr 0.000522, consume 74.38s
step 21500, loss 3.54, lr 0.000521, consume 74.39s
step 21600, loss 3.54, lr 0.000520, consume 74.38s
processing 16: english_c4/c4-train.00016-of-01024.txt, origin: 355697, samples: 173354, accum_tokens: 3020M, iter_num: 21687
step 21700, loss 3.54, lr 0.000519, consume 95.97s
step 21800, loss 3.55, lr 0.000519, consume 75.76s
step 21900, loss 3.54, lr 0.000518, consume 96.44s
step 22000, loss 3.55, lr 0.000517, consume 74.28s
step 22100, loss 3.54, lr 0.000516, consume 74.38s
step 22200, loss 3.54, lr 0.000515, consume 74.40s
step 22300, loss 3.54, lr 0.000515, consume 74.38s
step 22400, loss 3.54, lr 0.000514, consume 74.39s
step 22500, loss 3.54, lr 0.000513, consume 74.39s
step 22600, loss 3.54, lr 0.000512, consume 74.40s
step 22700, loss 3.53, lr 0.000511, consume 74.41s
step 22800, loss 3.54, lr 0.000511, consume 74.39s
step 22900, loss 3.54, lr 0.000510, consume 74.37s
step 23000, loss 3.54, lr 0.000509, consume 74.37s
processing 17: english_c4/c4-train.00017-of-01024.txt, origin: 355726, samples: 174259, accum_tokens: 3198M, iter_num: 23042
step 23100, loss 3.53, lr 0.000508, consume 94.65s
step 23200, loss 3.54, lr 0.000507, consume 77.50s
step 23300, loss 3.54, lr 0.000507, consume 94.22s
step 23400, loss 3.54, lr 0.000506, consume 74.39s
step 23500, loss 3.54, lr 0.000505, consume 74.42s
step 23600, loss 3.53, lr 0.000504, consume 74.42s
step 23700, loss 3.53, lr 0.000503, consume 74.41s
step 23800, loss 3.54, lr 0.000502, consume 74.40s
step 23900, loss 3.53, lr 0.000502, consume 74.41s
step 24000, loss 3.53, lr 0.000501, consume 74.40s
step 24100, loss 3.53, lr 0.000500, consume 74.49s
step 24200, loss 3.53, lr 0.000499, consume 74.88s
step 24300, loss 3.53, lr 0.000498, consume 74.63s
step 24400, loss 3.53, lr 0.000497, consume 75.87s
processing 18: english_c4/c4-train.00018-of-01024.txt, origin: 355709, samples: 173222, accum_tokens: 3376M, iter_num: 24403
step 24500, loss 3.54, lr 0.000497, consume 98.10s
step 24600, loss 3.54, lr 0.000496, consume 94.77s
step 24700, loss 3.53, lr 0.000495, consume 74.26s
step 24800, loss 3.54, lr 0.000494, consume 74.39s
step 24900, loss 3.54, lr 0.000493, consume 74.42s
step 25000, loss 3.53, lr 0.000492, consume 74.39s
step 25100, loss 3.53, lr 0.000491, consume 74.39s
step 25200, loss 3.53, lr 0.000490, consume 74.39s
step 25300, loss 3.53, lr 0.000490, consume 74.39s
step 25400, loss 3.53, lr 0.000489, consume 74.39s
step 25500, loss 3.52, lr 0.000488, consume 74.37s
step 25600, loss 3.51, lr 0.000487, consume 74.35s
step 25700, loss 3.52, lr 0.000486, consume 74.35s
processing 19: english_c4/c4-train.00019-of-01024.txt, origin: 355697, samples: 173993, accum_tokens: 3554M, iter_num: 25756
step 25800, loss 3.52, lr 0.000485, consume 97.17s
step 25900, loss 3.52, lr 0.000484, consume 76.73s
step 26000, loss 3.53, lr 0.000483, consume 96.05s
step 26100, loss 3.52, lr 0.000482, consume 74.32s
step 26200, loss 3.53, lr 0.000482, consume 74.38s
step 26300, loss 3.52, lr 0.000481, consume 74.37s
step 26400, loss 3.52, lr 0.000480, consume 74.36s
step 26500, loss 3.52, lr 0.000479, consume 74.34s
step 26600, loss 3.52, lr 0.000478, consume 74.34s
step 26700, loss 3.52, lr 0.000477, consume 74.34s
step 26800, loss 3.52, lr 0.000476, consume 74.35s
step 26900, loss 3.52, lr 0.000475, consume 74.33s
step 27000, loss 3.52, lr 0.000474, consume 74.35s
step 27100, loss 3.52, lr 0.000473, consume 74.35s
processing 20: english_c4/c4-train.00020-of-01024.txt, origin: 355687, samples: 173497, accum_tokens: 3732M, iter_num: 27115
step 27200, loss 3.53, lr 0.000472, consume 95.59s
step 27300, loss 3.53, lr 0.000472, consume 76.90s
step 27400, loss 3.53, lr 0.000471, consume 92.02s
step 27500, loss 3.52, lr 0.000470, consume 74.37s
step 27600, loss 3.52, lr 0.000469, consume 74.37s
step 27700, loss 3.52, lr 0.000468, consume 74.36s
step 27800, loss 3.51, lr 0.000467, consume 74.37s
step 27900, loss 3.52, lr 0.000466, consume 74.84s
step 28000, loss 3.52, lr 0.000465, consume 74.48s
step 28100, loss 3.51, lr 0.000464, consume 74.38s
step 28200, loss 3.51, lr 0.000463, consume 74.39s
step 28300, loss 3.52, lr 0.000462, consume 74.38s
step 28400, loss 3.51, lr 0.000461, consume 74.37s
processing 21: english_c4/c4-train.00021-of-01024.txt, origin: 355709, samples: 173358, accum_tokens: 3909M, iter_num: 28471
step 28500, loss 3.51, lr 0.000460, consume 96.62s
step 28600, loss 3.51, lr 0.000459, consume 76.03s
step 28700, loss 3.51, lr 0.000458, consume 95.19s
step 28800, loss 3.51, lr 0.000457, consume 74.32s
step 28900, loss 3.51, lr 0.000456, consume 74.38s
step 29000, loss 3.51, lr 0.000455, consume 74.40s
step 29100, loss 3.51, lr 0.000455, consume 74.40s
step 29200, loss 3.51, lr 0.000454, consume 74.39s
step 29300, loss 3.51, lr 0.000453, consume 74.38s
step 29400, loss 3.51, lr 0.000452, consume 74.40s
step 29500, loss 3.50, lr 0.000451, consume 74.37s
step 29600, loss 3.51, lr 0.000450, consume 74.40s
step 29700, loss 3.50, lr 0.000449, consume 74.39s
step 29800, loss 3.50, lr 0.000448, consume 74.38s
processing 22: english_c4/c4-train.00022-of-01024.txt, origin: 355742, samples: 173215, accum_tokens: 4086M, iter_num: 29825
step 29900, loss 3.52, lr 0.000447, consume 95.23s
step 30000, loss 3.51, lr 0.000446, consume 77.63s
step 30100, loss 3.51, lr 0.000445, consume 94.24s
step 30200, loss 3.51, lr 0.000444, consume 74.36s
step 30300, loss 3.51, lr 0.000443, consume 74.40s
step 30400, loss 3.52, lr 0.000442, consume 74.40s
step 30500, loss 3.50, lr 0.000441, consume 74.41s
step 30600, loss 3.50, lr 0.000440, consume 74.39s
step 30700, loss 3.51, lr 0.000439, consume 74.41s
step 30800, loss 3.51, lr 0.000438, consume 74.40s
step 30900, loss 3.51, lr 0.000437, consume 74.42s
step 31000, loss 3.51, lr 0.000436, consume 74.40s
step 31100, loss 3.50, lr 0.000435, consume 74.41s
processing 23: english_c4/c4-train.00023-of-01024.txt, origin: 355714, samples: 173979, accum_tokens: 4265M, iter_num: 31178
step 31200, loss 3.50, lr 0.000434, consume 96.95s
step 31300, loss 3.51, lr 0.000433, consume 75.32s
step 31400, loss 3.51, lr 0.000432, consume 94.20s
step 31500, loss 3.50, lr 0.000431, consume 74.49s
step 31600, loss 3.51, lr 0.000430, consume 74.46s
step 31700, loss 3.49, lr 0.000429, consume 74.43s
step 31800, loss 3.50, lr 0.000428, consume 74.41s
step 31900, loss 3.50, lr 0.000427, consume 74.39s
step 32000, loss 3.49, lr 0.000426, consume 74.38s
step 32100, loss 3.50, lr 0.000425, consume 74.37s
step 32200, loss 3.49, lr 0.000424, consume 74.38s
step 32300, loss 3.50, lr 0.000423, consume 74.39s
step 32400, loss 3.50, lr 0.000422, consume 74.39s
step 32500, loss 3.50, lr 0.000421, consume 74.38s
processing 24: english_c4/c4-train.00024-of-01024.txt, origin: 355728, samples: 172879, accum_tokens: 4442M, iter_num: 32537
step 32600, loss 3.50, lr 0.000420, consume 97.12s
step 32700, loss 3.50, lr 0.000419, consume 77.23s
step 32800, loss 3.50, lr 0.000418, consume 94.23s
step 32900, loss 3.50, lr 0.000417, consume 74.37s
step 33000, loss 3.50, lr 0.000416, consume 74.40s
step 33100, loss 3.50, lr 0.000414, consume 74.40s
step 33200, loss 3.50, lr 0.000413, consume 74.38s
step 33300, loss 3.50, lr 0.000412, consume 74.37s
step 33400, loss 3.49, lr 0.000411, consume 74.37s
step 33500, loss 3.50, lr 0.000410, consume 74.34s
step 33600, loss 3.50, lr 0.000409, consume 74.37s
step 33700, loss 3.50, lr 0.000408, consume 74.36s
step 33800, loss 3.49, lr 0.000407, consume 74.36s
processing 25: english_c4/c4-train.00025-of-01024.txt, origin: 355713, samples: 173638, accum_tokens: 4619M, iter_num: 33887
step 33900, loss 3.49, lr 0.000406, consume 95.79s
step 34000, loss 3.50, lr 0.000405, consume 75.85s
step 34100, loss 3.49, lr 0.000404, consume 95.34s
step 34200, loss 3.49, lr 0.000403, consume 74.26s
step 34300, loss 3.49, lr 0.000402, consume 74.38s
step 34400, loss 3.48, lr 0.000401, consume 74.39s
step 34500, loss 3.49, lr 0.000400, consume 74.38s
step 34600, loss 3.48, lr 0.000399, consume 74.37s
step 34700, loss 3.49, lr 0.000398, consume 74.36s
step 34800, loss 3.49, lr 0.000397, consume 74.36s
step 34900, loss 3.49, lr 0.000396, consume 74.37s
step 35000, loss 3.48, lr 0.000395, consume 74.37s
step 35100, loss 3.48, lr 0.000394, consume 74.37s
step 35200, loss 3.48, lr 0.000393, consume 74.37s
processing 26: english_c4/c4-train.00026-of-01024.txt, origin: 355688, samples: 173072, accum_tokens: 4797M, iter_num: 35244
step 35300, loss 3.49, lr 0.000391, consume 96.01s
step 35400, loss 3.49, lr 0.000390, consume 76.13s
step 35500, loss 3.48, lr 0.000389, consume 95.20s
step 35600, loss 3.49, lr 0.000388, consume 74.34s
step 35700, loss 3.49, lr 0.000387, consume 74.40s
step 35800, loss 3.48, lr 0.000386, consume 74.77s
step 35900, loss 3.48, lr 0.000385, consume 74.66s
step 36000, loss 3.48, lr 0.000384, consume 74.65s
step 36100, loss 3.48, lr 0.000383, consume 74.65s
step 36200, loss 3.48, lr 0.000382, consume 74.75s
step 36300, loss 3.48, lr 0.000381, consume 74.62s
step 36400, loss 3.48, lr 0.000380, consume 74.65s
step 36500, loss 3.48, lr 0.000379, consume 74.65s
processing 27: english_c4/c4-train.00027-of-01024.txt, origin: 355713, samples: 174471, accum_tokens: 4975M, iter_num: 36596
step 36600, loss 3.48, lr 0.000378, consume 95.77s
step 36700, loss 3.48, lr 0.000377, consume 76.26s
step 36800, loss 3.48, lr 0.000375, consume 95.63s
step 36900, loss 3.48, lr 0.000374, consume 74.54s
step 37000, loss 3.48, lr 0.000373, consume 74.64s
step 37100, loss 3.48, lr 0.000372, consume 74.63s
step 37200, loss 3.47, lr 0.000371, consume 74.63s
step 37300, loss 3.48, lr 0.000370, consume 74.63s
step 37400, loss 3.48, lr 0.000369, consume 74.64s
step 37500, loss 3.47, lr 0.000368, consume 74.62s
step 37600, loss 3.47, lr 0.000367, consume 74.61s
step 37700, loss 3.48, lr 0.000366, consume 74.61s
step 37800, loss 3.47, lr 0.000365, consume 74.57s
step 37900, loss 3.47, lr 0.000364, consume 74.57s
processing 28: english_c4/c4-train.00028-of-01024.txt, origin: 355701, samples: 173576, accum_tokens: 5153M, iter_num: 37959
step 38000, loss 3.48, lr 0.000363, consume 97.35s
step 38100, loss 3.48, lr 0.000361, consume 76.41s
step 38200, loss 3.48, lr 0.000360, consume 95.90s
step 38300, loss 3.48, lr 0.000359, consume 74.59s
step 38400, loss 3.47, lr 0.000358, consume 74.62s
step 38500, loss 3.48, lr 0.000357, consume 74.62s
step 38600, loss 3.47, lr 0.000356, consume 74.60s
step 38700, loss 3.47, lr 0.000355, consume 74.58s
step 38800, loss 3.47, lr 0.000354, consume 74.56s
step 38900, loss 3.47, lr 0.000353, consume 74.53s
step 39000, loss 3.47, lr 0.000352, consume 74.54s
step 39100, loss 3.47, lr 0.000351, consume 74.51s
step 39200, loss 3.47, lr 0.000350, consume 74.55s
step 39300, loss 3.47, lr 0.000348, consume 74.57s
processing 29: english_c4/c4-train.00029-of-01024.txt, origin: 355707, samples: 173593, accum_tokens: 5331M, iter_num: 39315
step 39400, loss 3.47, lr 0.000347, consume 97.00s
step 39500, loss 3.46, lr 0.000346, consume 96.61s
step 39600, loss 3.47, lr 0.000345, consume 74.32s
step 39700, loss 3.47, lr 0.000344, consume 74.51s
step 39800, loss 3.46, lr 0.000343, consume 74.55s
step 39900, loss 3.46, lr 0.000342, consume 74.53s
step 40000, loss 3.46, lr 0.000341, consume 74.54s
step 40100, loss 3.46, lr 0.000340, consume 74.54s
step 40200, loss 3.47, lr 0.000339, consume 74.53s
step 40300, loss 3.46, lr 0.000338, consume 74.53s
step 40400, loss 3.46, lr 0.000337, consume 74.55s
step 40500, loss 3.46, lr 0.000335, consume 74.58s
step 40600, loss 3.46, lr 0.000334, consume 74.60s
processing 30: english_c4/c4-train.00030-of-01024.txt, origin: 355751, samples: 173202, accum_tokens: 5508M, iter_num: 40671
step 40700, loss 3.47, lr 0.000333, consume 94.47s
step 40800, loss 3.47, lr 0.000332, consume 76.17s
step 40900, loss 3.46, lr 0.000331, consume 95.04s
step 41000, loss 3.47, lr 0.000330, consume 74.54s
step 41100, loss 3.46, lr 0.000329, consume 74.63s
step 41200, loss 3.46, lr 0.000328, consume 74.64s
step 41300, loss 3.46, lr 0.000327, consume 74.63s
step 41400, loss 3.46, lr 0.000326, consume 74.62s
step 41500, loss 3.46, lr 0.000325, consume 74.60s
step 41600, loss 3.46, lr 0.000323, consume 74.60s
step 41700, loss 3.46, lr 0.000322, consume 74.61s
step 41800, loss 3.46, lr 0.000321, consume 74.59s
step 41900, loss 3.46, lr 0.000320, consume 74.59s
step 42000, loss 3.45, lr 0.000319, consume 74.60s
processing 31: english_c4/c4-train.00031-of-01024.txt, origin: 355742, samples: 173547, accum_tokens: 5686M, iter_num: 42024
step 42100, loss 3.47, lr 0.000318, consume 96.80s
step 42200, loss 3.46, lr 0.000317, consume 78.01s
step 42300, loss 3.46, lr 0.000316, consume 92.09s
step 42400, loss 3.47, lr 0.000315, consume 74.61s
step 42500, loss 3.46, lr 0.000314, consume 74.61s
step 42600, loss 3.45, lr 0.000313, consume 74.62s
step 42700, loss 3.45, lr 0.000312, consume 74.62s
step 42800, loss 3.46, lr 0.000310, consume 74.61s
step 42900, loss 3.45, lr 0.000309, consume 74.59s
step 43000, loss 3.46, lr 0.000308, consume 74.56s
step 43100, loss 3.46, lr 0.000307, consume 74.59s
step 43200, loss 3.45, lr 0.000306, consume 74.59s
step 43300, loss 3.45, lr 0.000305, consume 74.59s
processing 32: english_c4/c4-train.00032-of-01024.txt, origin: 355738, samples: 174581, accum_tokens: 5865M, iter_num: 43380
step 43400, loss 3.45, lr 0.000304, consume 97.45s
step 43500, loss 3.45, lr 0.000303, consume 76.80s
step 43600, loss 3.46, lr 0.000302, consume 96.07s
step 43700, loss 3.45, lr 0.000301, consume 74.54s
step 43800, loss 3.45, lr 0.000300, consume 74.62s
step 43900, loss 3.45, lr 0.000299, consume 74.62s
step 44000, loss 3.45, lr 0.000297, consume 74.61s
step 44100, loss 3.45, lr 0.000296, consume 74.62s
step 44200, loss 3.46, lr 0.000295, consume 74.63s
step 44300, loss 3.45, lr 0.000294, consume 74.62s
step 44400, loss 3.45, lr 0.000293, consume 74.76s
step 44500, loss 3.45, lr 0.000292, consume 74.58s
step 44600, loss 3.45, lr 0.000291, consume 74.61s
step 44700, loss 3.45, lr 0.000290, consume 74.59s
processing 33: english_c4/c4-train.00033-of-01024.txt, origin: 355671, samples: 174239, accum_tokens: 6043M, iter_num: 44743
step 44800, loss 3.45, lr 0.000289, consume 97.53s
step 44900, loss 3.45, lr 0.000288, consume 77.26s
step 45000, loss 3.44, lr 0.000287, consume 94.37s
step 45100, loss 3.45, lr 0.000286, consume 74.60s
step 45200, loss 3.46, lr 0.000285, consume 74.63s
step 45300, loss 3.45, lr 0.000283, consume 74.64s
step 45400, loss 3.45, lr 0.000282, consume 74.61s
step 45500, loss 3.45, lr 0.000281, consume 74.60s
step 45600, loss 3.45, lr 0.000280, consume 74.59s
step 45700, loss 3.44, lr 0.000279, consume 74.55s
step 45800, loss 3.44, lr 0.000278, consume 74.55s
step 45900, loss 3.44, lr 0.000277, consume 74.56s
step 46000, loss 3.45, lr 0.000276, consume 74.60s
step 46100, loss 3.44, lr 0.000275, consume 74.70s
processing 34: english_c4/c4-train.00034-of-01024.txt, origin: 355742, samples: 173538, accum_tokens: 6221M, iter_num: 46104
step 46200, loss 3.45, lr 0.000274, consume 96.13s
step 46300, loss 3.45, lr 0.000273, consume 95.04s
step 46400, loss 3.44, lr 0.000272, consume 74.46s
step 46500, loss 3.44, lr 0.000271, consume 74.62s
step 46600, loss 3.45, lr 0.000270, consume 74.64s
step 46700, loss 3.44, lr 0.000269, consume 74.62s
step 46800, loss 3.44, lr 0.000268, consume 74.61s
step 46900, loss 3.44, lr 0.000266, consume 74.63s
step 47000, loss 3.44, lr 0.000265, consume 74.60s
step 47100, loss 3.44, lr 0.000264, consume 74.60s
step 47200, loss 3.43, lr 0.000263, consume 74.60s
step 47300, loss 3.44, lr 0.000262, consume 74.59s
step 47400, loss 3.44, lr 0.000261, consume 74.67s
processing 35: english_c4/c4-train.00035-of-01024.txt, origin: 355715, samples: 173348, accum_tokens: 6398M, iter_num: 47460
step 47500, loss 3.44, lr 0.000260, consume 95.90s
step 47600, loss 3.45, lr 0.000259, consume 76.42s
step 47700, loss 3.44, lr 0.000258, consume 95.74s
step 47800, loss 3.45, lr 0.000257, consume 74.60s
step 47900, loss 3.44, lr 0.000256, consume 74.64s
step 48000, loss 3.44, lr 0.000255, consume 74.63s
step 48100, loss 3.44, lr 0.000254, consume 74.62s
step 48200, loss 3.44, lr 0.000253, consume 74.64s
step 48300, loss 3.43, lr 0.000252, consume 74.62s
step 48400, loss 3.43, lr 0.000251, consume 74.60s
step 48500, loss 3.44, lr 0.000250, consume 74.60s
step 48600, loss 3.44, lr 0.000249, consume 74.60s
step 48700, loss 3.44, lr 0.000248, consume 74.59s
step 48800, loss 3.43, lr 0.000247, consume 74.60s
processing 36: english_c4/c4-train.00036-of-01024.txt, origin: 355725, samples: 173573, accum_tokens: 6576M, iter_num: 48814
step 48900, loss 3.43, lr 0.000246, consume 96.46s
step 49000, loss 3.43, lr 0.000245, consume 96.22s
step 49100, loss 3.43, lr 0.000243, consume 74.40s
step 49200, loss 3.43, lr 0.000242, consume 74.62s
step 49300, loss 3.43, lr 0.000241, consume 74.62s
step 49400, loss 3.43, lr 0.000240, consume 74.72s
step 49500, loss 3.43, lr 0.000239, consume 74.59s
step 49600, loss 3.43, lr 0.000238, consume 74.62s
step 49700, loss 3.44, lr 0.000237, consume 74.63s
step 49800, loss 3.43, lr 0.000236, consume 74.63s
step 49900, loss 3.42, lr 0.000235, consume 74.64s
step 50000, loss 3.43, lr 0.000234, consume 74.64s
step 50100, loss 3.43, lr 0.000233, consume 74.62s
processing 37: english_c4/c4-train.00037-of-01024.txt, origin: 355760, samples: 174025, accum_tokens: 6754M, iter_num: 50170
step 50200, loss 3.42, lr 0.000232, consume 95.66s
step 50300, loss 3.43, lr 0.000231, consume 76.78s
step 50400, loss 3.43, lr 0.000230, consume 95.61s
step 50500, loss 3.43, lr 0.000229, consume 74.59s
step 50600, loss 3.43, lr 0.000228, consume 74.65s
step 50700, loss 3.44, lr 0.000227, consume 74.63s
step 50800, loss 3.43, lr 0.000226, consume 74.62s
step 50900, loss 3.43, lr 0.000225, consume 74.95s
step 51000, loss 3.43, lr 0.000224, consume 74.61s
step 51100, loss 3.43, lr 0.000223, consume 74.62s
step 51200, loss 3.43, lr 0.000222, consume 74.61s
step 51300, loss 3.43, lr 0.000221, consume 74.60s
step 51400, loss 3.43, lr 0.000220, consume 74.62s
step 51500, loss 3.42, lr 0.000219, consume 74.59s
processing 38: english_c4/c4-train.00038-of-01024.txt, origin: 355686, samples: 173172, accum_tokens: 6932M, iter_num: 51530
step 51600, loss 3.43, lr 0.000218, consume 96.05s
step 51700, loss 3.43, lr 0.000217, consume 76.92s
step 51800, loss 3.42, lr 0.000216, consume 94.26s
step 51900, loss 3.42, lr 0.000215, consume 74.58s
step 52000, loss 3.42, lr 0.000214, consume 74.60s
step 52100, loss 3.42, lr 0.000213, consume 74.62s
step 52200, loss 3.42, lr 0.000212, consume 74.61s
step 52300, loss 3.41, lr 0.000211, consume 74.60s
step 52400, loss 3.42, lr 0.000210, consume 74.59s
step 52500, loss 3.42, lr 0.000209, consume 74.58s
step 52600, loss 3.42, lr 0.000208, consume 74.57s
step 52700, loss 3.42, lr 0.000207, consume 74.57s
step 52800, loss 3.42, lr 0.000206, consume 74.59s
processing 39: english_c4/c4-train.00039-of-01024.txt, origin: 355723, samples: 174130, accum_tokens: 7110M, iter_num: 52883
step 52900, loss 3.41, lr 0.000205, consume 95.59s
step 53000, loss 3.43, lr 0.000205, consume 76.04s
step 53100, loss 3.43, lr 0.000204, consume 94.70s
step 53200, loss 3.43, lr 0.000203, consume 74.48s
step 53300, loss 3.42, lr 0.000202, consume 74.58s
step 53400, loss 3.42, lr 0.000201, consume 74.62s
step 53500, loss 3.41, lr 0.000200, consume 74.62s
step 53600, loss 3.41, lr 0.000199, consume 74.60s
step 53700, loss 3.43, lr 0.000198, consume 74.57s
step 53800, loss 3.43, lr 0.000197, consume 74.57s
step 53900, loss 3.42, lr 0.000196, consume 74.58s
step 54000, loss 3.42, lr 0.000195, consume 74.58s
step 54100, loss 3.42, lr 0.000194, consume 74.60s
step 54200, loss 3.42, lr 0.000193, consume 74.59s
processing 40: english_c4/c4-train.00040-of-01024.txt, origin: 355743, samples: 174205, accum_tokens: 7288M, iter_num: 54243
step 54300, loss 3.42, lr 0.000192, consume 97.18s
step 54400, loss 3.42, lr 0.000191, consume 76.58s
step 54500, loss 3.42, lr 0.000190, consume 95.84s
step 54600, loss 3.42, lr 0.000189, consume 74.56s
step 54700, loss 3.42, lr 0.000188, consume 74.58s
step 54800, loss 3.42, lr 0.000188, consume 74.61s
step 54900, loss 3.42, lr 0.000187, consume 74.63s
step 55000, loss 3.41, lr 0.000186, consume 74.61s
step 55100, loss 3.41, lr 0.000185, consume 74.59s
step 55200, loss 3.41, lr 0.000184, consume 74.58s
step 55300, loss 3.41, lr 0.000183, consume 74.59s
step 55400, loss 3.41, lr 0.000182, consume 74.57s
step 55500, loss 3.42, lr 0.000181, consume 74.58s
step 55600, loss 3.42, lr 0.000180, consume 74.57s
processing 41: english_c4/c4-train.00041-of-01024.txt, origin: 355732, samples: 173841, accum_tokens: 7466M, iter_num: 55604
step 55700, loss 3.42, lr 0.000179, consume 97.01s
step 55800, loss 3.42, lr 0.000178, consume 94.57s
step 55900, loss 3.41, lr 0.000178, consume 74.45s
step 56000, loss 3.42, lr 0.000177, consume 74.58s
step 56100, loss 3.41, lr 0.000176, consume 74.61s
step 56200, loss 3.41, lr 0.000175, consume 74.73s
step 56300, loss 3.41, lr 0.000174, consume 74.43s
step 56400, loss 3.41, lr 0.000173, consume 74.70s
step 56500, loss 3.41, lr 0.000172, consume 75.04s
step 56600, loss 3.41, lr 0.000171, consume 75.65s
step 56700, loss 3.41, lr 0.000170, consume 76.84s
step 56800, loss 3.41, lr 0.000170, consume 76.31s
step 56900, loss 3.41, lr 0.000169, consume 76.41s
processing 42: english_c4/c4-train.00042-of-01024.txt, origin: 355693, samples: 172927, accum_tokens: 7643M, iter_num: 56962
step 57000, loss 3.41, lr 0.000168, consume 98.20s
step 57100, loss 3.41, lr 0.000167, consume 77.69s
step 57200, loss 3.41, lr 0.000166, consume 95.55s
step 57300, loss 3.41, lr 0.000165, consume 75.02s
step 57400, loss 3.41, lr 0.000164, consume 74.59s
step 57500, loss 3.41, lr 0.000163, consume 74.37s
step 57600, loss 3.41, lr 0.000163, consume 74.36s
step 57700, loss 3.41, lr 0.000162, consume 74.39s
step 57800, loss 3.41, lr 0.000161, consume 74.81s
step 57900, loss 3.41, lr 0.000160, consume 76.34s
step 58000, loss 3.40, lr 0.000159, consume 75.50s
step 58100, loss 3.41, lr 0.000158, consume 75.25s
step 58200, loss 3.41, lr 0.000158, consume 75.53s
step 58300, loss 3.41, lr 0.000157, consume 75.08s
processing 43: english_c4/c4-train.00043-of-01024.txt, origin: 355728, samples: 172642, accum_tokens: 7820M, iter_num: 58312
step 58400, loss 3.41, lr 0.000156, consume 97.03s
step 58500, loss 3.41, lr 0.000155, consume 97.41s
step 58600, loss 3.40, lr 0.000154, consume 75.46s
step 58700, loss 3.40, lr 0.000153, consume 76.51s
step 58800, loss 3.41, lr 0.000153, consume 75.85s
step 58900, loss 3.40, lr 0.000152, consume 75.45s
step 59000, loss 3.41, lr 0.000151, consume 75.56s
step 59100, loss 3.40, lr 0.000150, consume 76.37s
step 59200, loss 3.40, lr 0.000149, consume 74.99s
step 59300, loss 3.41, lr 0.000149, consume 74.37s
step 59400, loss 3.40, lr 0.000148, consume 74.86s
step 59500, loss 3.40, lr 0.000147, consume 76.57s
step 59600, loss 3.40, lr 0.000146, consume 74.69s
processing 44: english_c4/c4-train.00044-of-01024.txt, origin: 355718, samples: 173521, accum_tokens: 7998M, iter_num: 59661
step 59700, loss 3.40, lr 0.000145, consume 95.18s
step 59800, loss 3.40, lr 0.000145, consume 76.03s
step 59900, loss 3.40, lr 0.000144, consume 95.75s
step 60000, loss 3.40, lr 0.000143, consume 75.04s
step 60100, loss 3.40, lr 0.000142, consume 74.66s
step 60200, loss 3.40, lr 0.000141, consume 74.36s
step 60300, loss 3.40, lr 0.000141, consume 74.37s
step 60400, loss 3.39, lr 0.000140, consume 74.35s
step 60500, loss 3.40, lr 0.000139, consume 74.35s
