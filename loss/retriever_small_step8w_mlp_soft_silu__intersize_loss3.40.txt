RerieverConfig_small()
	batch_size: 32
	beta1: 0.9
	beta2: 0.95
	grad_clip: 1.0
	gradient_accumulation_steps: 4
	hidden_size: 512
	learning_rate: 0.0006
	lr_decay_iters: 80000
	max_iters: 80000
	min_lr: 6e-05
	num_heads: 8
	num_layers: 6
	sequence_length: 1024
	vocab_size: 32000
	warmup_iters: 2000
	weight_decay: 0.1

OptimizedModule(
  (_orig_mod): Retriever(
    (token_embedding): Embedding(32000, 512)
    (position_embedding): Embedding(1024, 512)
    (layers): ModuleList(
      (0-5): 6 x DecoderLayer(
        (ln_1): RMSNorm()
        (attn): Attention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
        )
        (ln_2): RMSNorm()
        (mlp): MLP(
          (gate_proj): Linear(in_features=512, out_features=1372, bias=False)
          (up_proj): Linear(in_features=512, out_features=1372, bias=False)
          (down_proj): Linear(in_features=1372, out_features=512, bias=False)
        )
      )
    )
    (norm): RMSNorm()
    (lm_head): Linear(in_features=512, out_features=32000, bias=False)
  )
)

num decayed parameter tensors: 44, with 35,844,096 parameters
num non-decayed parameter tensors: 13, with 6,656 parameters
number of total parameters: 35.33M
all train file nums: 1024
preparing first dataset file english_c4/c4-train.00000-of-01024.txt
finished, consume: 118s
processing 0: english_c4/c4-train.00000-of-01024.txt, origin: 355732, samples: 173721, accum_tokens: 177M, iter_num: 0
step 100, loss 9.70, lr 0.000030, consume 101.18s
step 200, loss 7.90, lr 0.000060, consume 94.04s
step 300, loss 6.83, lr 0.000090, consume 73.02s
step 400, loss 6.40, lr 0.000120, consume 73.26s
step 500, loss 6.18, lr 0.000150, consume 73.27s
step 600, loss 6.01, lr 0.000180, consume 73.30s
step 700, loss 5.85, lr 0.000210, consume 73.30s
step 800, loss 5.68, lr 0.000240, consume 73.29s
step 900, loss 5.53, lr 0.000270, consume 73.35s
step 1000, loss 5.39, lr 0.000300, consume 74.24s
step 1100, loss 5.26, lr 0.000330, consume 73.50s
step 1200, loss 5.13, lr 0.000360, consume 73.30s
step 1300, loss 5.02, lr 0.000390, consume 73.29s
processing 1: english_c4/c4-train.00001-of-01024.txt, origin: 355708, samples: 173229, accum_tokens: 355M, iter_num: 1357
step 1400, loss 4.90, lr 0.000420, consume 93.72s
step 1500, loss 4.76, lr 0.000450, consume 76.13s
step 1600, loss 4.64, lr 0.000480, consume 94.70s
step 1700, loss 4.56, lr 0.000510, consume 73.54s
step 1800, loss 4.48, lr 0.000540, consume 73.59s
step 1900, loss 4.42, lr 0.000570, consume 73.61s
step 2000, loss 4.36, lr 0.000600, consume 73.62s
step 2100, loss 4.32, lr 0.000600, consume 74.29s
step 2200, loss 4.27, lr 0.000600, consume 73.60s
step 2300, loss 4.24, lr 0.000600, consume 73.59s
step 2400, loss 4.21, lr 0.000600, consume 73.60s
step 2500, loss 4.18, lr 0.000600, consume 73.62s
step 2600, loss 4.15, lr 0.000600, consume 73.60s
step 2700, loss 4.12, lr 0.000600, consume 73.57s
processing 2: english_c4/c4-train.00002-of-01024.txt, origin: 355718, samples: 173903, accum_tokens: 533M, iter_num: 2710
step 2800, loss 4.11, lr 0.000600, consume 94.65s
step 2900, loss 4.09, lr 0.000600, consume 95.62s
step 3000, loss 4.08, lr 0.000600, consume 73.30s
step 3100, loss 4.05, lr 0.000600, consume 73.55s
step 3200, loss 4.04, lr 0.000600, consume 73.58s
step 3300, loss 4.03, lr 0.000600, consume 73.58s
step 3400, loss 4.00, lr 0.000600, consume 73.57s
step 3500, loss 3.99, lr 0.000600, consume 73.54s
step 3600, loss 3.98, lr 0.000599, consume 73.54s
step 3700, loss 3.96, lr 0.000599, consume 73.54s
step 3800, loss 3.95, lr 0.000599, consume 73.49s
step 3900, loss 3.94, lr 0.000599, consume 73.32s
step 4000, loss 3.94, lr 0.000599, consume 73.31s
processing 3: english_c4/c4-train.00003-of-01024.txt, origin: 355753, samples: 173350, accum_tokens: 710M, iter_num: 4068
step 4100, loss 3.93, lr 0.000599, consume 92.79s
step 4200, loss 3.92, lr 0.000599, consume 75.50s
step 4300, loss 3.90, lr 0.000599, consume 95.22s
step 4400, loss 3.91, lr 0.000599, consume 73.22s
step 4500, loss 3.91, lr 0.000599, consume 73.30s
step 4600, loss 3.89, lr 0.000599, consume 73.31s
step 4700, loss 3.88, lr 0.000598, consume 73.31s
step 4800, loss 3.87, lr 0.000598, consume 73.29s
step 4900, loss 3.87, lr 0.000598, consume 73.29s
step 5000, loss 3.86, lr 0.000598, consume 73.28s
step 5100, loss 3.85, lr 0.000598, consume 73.50s
step 5200, loss 3.85, lr 0.000598, consume 73.53s
step 5300, loss 3.85, lr 0.000598, consume 73.52s
step 5400, loss 3.84, lr 0.000597, consume 73.52s
processing 4: english_c4/c4-train.00004-of-01024.txt, origin: 355691, samples: 173171, accum_tokens: 888M, iter_num: 5423
step 5500, loss 3.84, lr 0.000597, consume 92.98s
step 5600, loss 3.84, lr 0.000597, consume 76.71s
step 5700, loss 3.83, lr 0.000597, consume 93.48s
step 5800, loss 3.82, lr 0.000597, consume 73.54s
step 5900, loss 3.83, lr 0.000597, consume 73.59s
step 6000, loss 3.82, lr 0.000597, consume 73.57s
step 6100, loss 3.81, lr 0.000596, consume 73.56s
step 6200, loss 3.81, lr 0.000596, consume 73.57s
step 6300, loss 3.79, lr 0.000596, consume 73.55s
step 6400, loss 3.80, lr 0.000596, consume 73.56s
step 6500, loss 3.80, lr 0.000596, consume 73.56s
step 6600, loss 3.80, lr 0.000595, consume 73.57s
step 6700, loss 3.78, lr 0.000595, consume 73.56s
processing 5: english_c4/c4-train.00005-of-01024.txt, origin: 355717, samples: 172927, accum_tokens: 1065M, iter_num: 6775
step 6800, loss 3.78, lr 0.000595, consume 92.86s
step 6900, loss 3.78, lr 0.000595, consume 74.50s
step 7000, loss 3.78, lr 0.000595, consume 96.27s
step 7100, loss 3.79, lr 0.000594, consume 73.45s
step 7200, loss 3.77, lr 0.000594, consume 73.60s
step 7300, loss 3.77, lr 0.000594, consume 73.60s
step 7400, loss 3.77, lr 0.000594, consume 73.58s
step 7500, loss 3.76, lr 0.000593, consume 73.55s
step 7600, loss 3.75, lr 0.000593, consume 73.54s
step 7700, loss 3.76, lr 0.000593, consume 73.54s
step 7800, loss 3.75, lr 0.000593, consume 73.55s
step 7900, loss 3.76, lr 0.000592, consume 73.55s
step 8000, loss 3.75, lr 0.000592, consume 73.55s
step 8100, loss 3.74, lr 0.000592, consume 73.55s
processing 6: english_c4/c4-train.00006-of-01024.txt, origin: 355747, samples: 173252, accum_tokens: 1242M, iter_num: 8126
step 8200, loss 3.75, lr 0.000592, consume 94.13s
step 8300, loss 3.74, lr 0.000591, consume 75.47s
step 8400, loss 3.73, lr 0.000591, consume 95.12s
step 8500, loss 3.73, lr 0.000591, consume 73.53s
step 8600, loss 3.73, lr 0.000591, consume 73.55s
step 8700, loss 3.73, lr 0.000590, consume 73.53s
step 8800, loss 3.72, lr 0.000590, consume 73.53s
step 8900, loss 3.72, lr 0.000590, consume 73.58s
step 9000, loss 3.72, lr 0.000589, consume 73.53s
step 9100, loss 3.72, lr 0.000589, consume 73.51s
step 9200, loss 3.72, lr 0.000589, consume 73.51s
step 9300, loss 3.71, lr 0.000588, consume 73.51s
step 9400, loss 3.71, lr 0.000588, consume 73.50s
processing 7: english_c4/c4-train.00007-of-01024.txt, origin: 355719, samples: 173288, accum_tokens: 1420M, iter_num: 9480
step 9500, loss 3.71, lr 0.000588, consume 92.84s
step 9600, loss 3.72, lr 0.000587, consume 74.51s
step 9700, loss 3.71, lr 0.000587, consume 96.07s
step 9800, loss 3.72, lr 0.000587, consume 73.45s
step 9900, loss 3.71, lr 0.000586, consume 73.57s
step 10000, loss 3.70, lr 0.000586, consume 73.60s
step 10100, loss 3.71, lr 0.000586, consume 73.59s
step 10200, loss 3.70, lr 0.000585, consume 73.60s
step 10300, loss 3.70, lr 0.000585, consume 73.59s
step 10400, loss 3.70, lr 0.000585, consume 73.57s
step 10500, loss 3.69, lr 0.000584, consume 73.55s
step 10600, loss 3.70, lr 0.000584, consume 73.65s
step 10700, loss 3.69, lr 0.000584, consume 73.54s
step 10800, loss 3.69, lr 0.000583, consume 73.53s
processing 8: english_c4/c4-train.00008-of-01024.txt, origin: 355746, samples: 173024, accum_tokens: 1597M, iter_num: 10833
step 10900, loss 3.70, lr 0.000583, consume 92.95s
step 11000, loss 3.68, lr 0.000582, consume 75.43s
step 11100, loss 3.69, lr 0.000582, consume 94.68s
step 11200, loss 3.69, lr 0.000582, consume 73.52s
step 11300, loss 3.68, lr 0.000581, consume 73.55s
step 11400, loss 3.69, lr 0.000581, consume 73.55s
step 11500, loss 3.69, lr 0.000580, consume 73.54s
step 11600, loss 3.68, lr 0.000580, consume 73.55s
step 11700, loss 3.68, lr 0.000580, consume 73.54s
step 11800, loss 3.68, lr 0.000579, consume 73.53s
step 11900, loss 3.68, lr 0.000579, consume 73.55s
step 12000, loss 3.68, lr 0.000578, consume 73.52s
step 12100, loss 3.68, lr 0.000578, consume 73.33s
processing 9: english_c4/c4-train.00009-of-01024.txt, origin: 355708, samples: 172984, accum_tokens: 1774M, iter_num: 12185
step 12200, loss 3.67, lr 0.000578, consume 92.77s
step 12300, loss 3.68, lr 0.000577, consume 74.22s
step 12400, loss 3.67, lr 0.000577, consume 95.83s
step 12500, loss 3.67, lr 0.000576, consume 73.19s
step 12600, loss 3.67, lr 0.000576, consume 73.40s
step 12700, loss 3.67, lr 0.000575, consume 73.40s
step 12800, loss 3.66, lr 0.000575, consume 73.40s
step 12900, loss 3.66, lr 0.000574, consume 73.40s
step 13000, loss 3.66, lr 0.000574, consume 73.39s
step 13100, loss 3.66, lr 0.000573, consume 73.38s
step 13200, loss 3.66, lr 0.000573, consume 73.39s
step 13300, loss 3.66, lr 0.000573, consume 73.38s
step 13400, loss 3.65, lr 0.000572, consume 73.37s
step 13500, loss 3.66, lr 0.000572, consume 73.37s
processing 10: english_c4/c4-train.00010-of-01024.txt, origin: 355702, samples: 173710, accum_tokens: 1952M, iter_num: 13536
step 13600, loss 3.66, lr 0.000571, consume 93.99s
step 13700, loss 3.66, lr 0.000571, consume 75.22s
step 13800, loss 3.65, lr 0.000570, consume 92.26s
step 13900, loss 3.65, lr 0.000570, consume 73.32s
step 14000, loss 3.65, lr 0.000569, consume 73.35s
step 14100, loss 3.65, lr 0.000569, consume 73.37s
step 14200, loss 3.65, lr 0.000568, consume 73.36s
step 14300, loss 3.65, lr 0.000568, consume 73.35s
step 14400, loss 3.65, lr 0.000567, consume 73.34s
step 14500, loss 3.64, lr 0.000567, consume 73.33s
step 14600, loss 3.64, lr 0.000566, consume 73.34s
step 14700, loss 3.65, lr 0.000565, consume 73.36s
step 14800, loss 3.64, lr 0.000565, consume 73.35s
processing 11: english_c4/c4-train.00011-of-01024.txt, origin: 355701, samples: 173885, accum_tokens: 2130M, iter_num: 14893
step 14900, loss 3.63, lr 0.000564, consume 94.10s
step 15000, loss 3.65, lr 0.000564, consume 75.04s
step 15100, loss 3.65, lr 0.000563, consume 94.35s
step 15200, loss 3.64, lr 0.000563, consume 73.17s
step 15300, loss 3.64, lr 0.000562, consume 73.35s
step 15400, loss 3.64, lr 0.000562, consume 73.36s
step 15500, loss 3.64, lr 0.000561, consume 73.36s
step 15600, loss 3.63, lr 0.000561, consume 73.34s
step 15700, loss 3.64, lr 0.000560, consume 73.34s
step 15800, loss 3.64, lr 0.000559, consume 73.34s
step 15900, loss 3.64, lr 0.000559, consume 73.32s
step 16000, loss 3.64, lr 0.000558, consume 73.32s
step 16100, loss 3.63, lr 0.000558, consume 73.31s
step 16200, loss 3.64, lr 0.000557, consume 73.33s
processing 12: english_c4/c4-train.00012-of-01024.txt, origin: 355710, samples: 174423, accum_tokens: 2308M, iter_num: 16252
step 16300, loss 3.63, lr 0.000556, consume 93.21s
step 16400, loss 3.64, lr 0.000556, consume 75.05s
step 16500, loss 3.63, lr 0.000555, consume 94.16s
step 16600, loss 3.62, lr 0.000555, consume 73.32s
step 16700, loss 3.63, lr 0.000554, consume 73.35s
step 16800, loss 3.62, lr 0.000553, consume 73.36s
step 16900, loss 3.63, lr 0.000553, consume 73.35s
step 17000, loss 3.63, lr 0.000552, consume 73.33s
step 17100, loss 3.63, lr 0.000552, consume 73.35s
step 17200, loss 3.63, lr 0.000551, consume 73.35s
step 17300, loss 3.62, lr 0.000550, consume 73.33s
step 17400, loss 3.61, lr 0.000550, consume 73.33s
step 17500, loss 3.61, lr 0.000549, consume 73.34s
step 17600, loss 3.62, lr 0.000548, consume 73.33s
processing 13: english_c4/c4-train.00013-of-01024.txt, origin: 355723, samples: 173966, accum_tokens: 2487M, iter_num: 17614
step 17700, loss 3.63, lr 0.000548, consume 93.45s
step 17800, loss 3.62, lr 0.000547, consume 76.43s
step 17900, loss 3.62, lr 0.000547, consume 93.49s
step 18000, loss 3.62, lr 0.000546, consume 73.33s
step 18100, loss 3.62, lr 0.000545, consume 73.35s
step 18200, loss 3.61, lr 0.000545, consume 73.34s
step 18300, loss 3.61, lr 0.000544, consume 73.34s
step 18400, loss 3.61, lr 0.000543, consume 73.35s
step 18500, loss 3.61, lr 0.000543, consume 73.34s
step 18600, loss 3.62, lr 0.000542, consume 73.34s
step 18700, loss 3.60, lr 0.000541, consume 73.35s
step 18800, loss 3.61, lr 0.000541, consume 73.33s
step 18900, loss 3.61, lr 0.000540, consume 73.34s
processing 14: english_c4/c4-train.00014-of-01024.txt, origin: 355708, samples: 174115, accum_tokens: 2665M, iter_num: 18973
step 19000, loss 3.61, lr 0.000539, consume 92.66s
step 19100, loss 3.61, lr 0.000538, consume 74.29s
step 19200, loss 3.61, lr 0.000538, consume 96.03s
step 19300, loss 3.60, lr 0.000537, consume 73.21s
step 19400, loss 3.61, lr 0.000536, consume 73.33s
step 19500, loss 3.61, lr 0.000536, consume 73.34s
step 19600, loss 3.61, lr 0.000535, consume 73.33s
step 19700, loss 3.60, lr 0.000534, consume 73.32s
step 19800, loss 3.61, lr 0.000534, consume 73.41s
step 19900, loss 3.59, lr 0.000533, consume 73.31s
step 20000, loss 3.61, lr 0.000532, consume 73.34s
step 20100, loss 3.61, lr 0.000531, consume 73.32s
step 20200, loss 3.60, lr 0.000531, consume 73.33s
step 20300, loss 3.59, lr 0.000530, consume 73.32s
processing 15: english_c4/c4-train.00015-of-01024.txt, origin: 355698, samples: 173319, accum_tokens: 2842M, iter_num: 20333
step 20400, loss 3.61, lr 0.000529, consume 94.03s
step 20500, loss 3.61, lr 0.000528, consume 75.32s
step 20600, loss 3.60, lr 0.000528, consume 94.81s
step 20700, loss 3.60, lr 0.000527, consume 73.34s
step 20800, loss 3.60, lr 0.000526, consume 73.37s
step 20900, loss 3.60, lr 0.000525, consume 73.36s
step 21000, loss 3.60, lr 0.000525, consume 73.38s
step 21100, loss 3.60, lr 0.000524, consume 73.37s
step 21200, loss 3.60, lr 0.000523, consume 73.34s
step 21300, loss 3.60, lr 0.000522, consume 73.33s
step 21400, loss 3.60, lr 0.000522, consume 73.36s
step 21500, loss 3.60, lr 0.000521, consume 73.36s
step 21600, loss 3.59, lr 0.000520, consume 73.36s
processing 16: english_c4/c4-train.00016-of-01024.txt, origin: 355697, samples: 173354, accum_tokens: 3020M, iter_num: 21687
step 21700, loss 3.59, lr 0.000519, consume 92.64s
step 21800, loss 3.59, lr 0.000519, consume 74.32s
step 21900, loss 3.60, lr 0.000518, consume 95.90s
step 22000, loss 3.59, lr 0.000517, consume 73.20s
step 22100, loss 3.61, lr 0.000516, consume 73.39s
step 22200, loss 3.59, lr 0.000515, consume 73.39s
step 22300, loss 3.59, lr 0.000515, consume 73.39s
step 22400, loss 3.60, lr 0.000514, consume 73.36s
step 22500, loss 3.59, lr 0.000513, consume 73.36s
step 22600, loss 3.59, lr 0.000512, consume 73.35s
step 22700, loss 3.59, lr 0.000511, consume 73.37s
step 22800, loss 3.58, lr 0.000511, consume 73.35s
step 22900, loss 3.58, lr 0.000510, consume 73.34s
step 23000, loss 3.59, lr 0.000509, consume 73.35s
processing 17: english_c4/c4-train.00017-of-01024.txt, origin: 355726, samples: 174259, accum_tokens: 3198M, iter_num: 23042
step 23100, loss 3.59, lr 0.000508, consume 92.81s
step 23200, loss 3.59, lr 0.000507, consume 75.26s
step 23300, loss 3.60, lr 0.000507, consume 94.87s
step 23400, loss 3.59, lr 0.000506, consume 73.32s
step 23500, loss 3.58, lr 0.000505, consume 73.35s
step 23600, loss 3.59, lr 0.000504, consume 73.36s
step 23700, loss 3.58, lr 0.000503, consume 73.35s
step 23800, loss 3.58, lr 0.000502, consume 73.34s
step 23900, loss 3.58, lr 0.000502, consume 73.34s
step 24000, loss 3.58, lr 0.000501, consume 73.34s
step 24100, loss 3.58, lr 0.000500, consume 73.35s
step 24200, loss 3.58, lr 0.000499, consume 73.61s
step 24300, loss 3.57, lr 0.000498, consume 74.06s
step 24400, loss 3.59, lr 0.000497, consume 73.58s
processing 18: english_c4/c4-train.00018-of-01024.txt, origin: 355709, samples: 173222, accum_tokens: 3376M, iter_num: 24403
step 24500, loss 3.59, lr 0.000497, consume 93.76s
step 24600, loss 3.58, lr 0.000496, consume 96.42s
step 24700, loss 3.58, lr 0.000495, consume 73.36s
step 24800, loss 3.58, lr 0.000494, consume 73.59s
step 24900, loss 3.57, lr 0.000493, consume 73.59s
step 25000, loss 3.58, lr 0.000492, consume 73.62s
step 25100, loss 3.58, lr 0.000491, consume 73.61s
step 25200, loss 3.58, lr 0.000490, consume 73.59s
step 25300, loss 3.58, lr 0.000490, consume 73.60s
step 25400, loss 3.57, lr 0.000489, consume 73.60s
step 25500, loss 3.58, lr 0.000488, consume 73.60s
step 25600, loss 3.57, lr 0.000487, consume 73.61s
step 25700, loss 3.57, lr 0.000486, consume 73.62s
processing 19: english_c4/c4-train.00019-of-01024.txt, origin: 355697, samples: 173993, accum_tokens: 3554M, iter_num: 25756
step 25800, loss 3.58, lr 0.000485, consume 93.87s
step 25900, loss 3.58, lr 0.000484, consume 75.66s
step 26000, loss 3.57, lr 0.000483, consume 92.65s
step 26100, loss 3.57, lr 0.000482, consume 73.54s
step 26200, loss 3.57, lr 0.000482, consume 73.59s
step 26300, loss 3.58, lr 0.000481, consume 73.57s
step 26400, loss 3.58, lr 0.000480, consume 73.58s
step 26500, loss 3.56, lr 0.000479, consume 73.71s
step 26600, loss 3.57, lr 0.000478, consume 73.72s
step 26700, loss 3.56, lr 0.000477, consume 73.72s
step 26800, loss 3.57, lr 0.000476, consume 73.63s
step 26900, loss 3.56, lr 0.000475, consume 73.57s
step 27000, loss 3.56, lr 0.000474, consume 73.59s
step 27100, loss 3.56, lr 0.000473, consume 73.90s
processing 20: english_c4/c4-train.00020-of-01024.txt, origin: 355687, samples: 173497, accum_tokens: 3732M, iter_num: 27115
step 27200, loss 3.58, lr 0.000472, consume 95.32s
step 27300, loss 3.57, lr 0.000472, consume 75.66s
step 27400, loss 3.57, lr 0.000471, consume 92.90s
step 27500, loss 3.57, lr 0.000470, consume 73.34s
step 27600, loss 3.57, lr 0.000469, consume 73.38s
step 27700, loss 3.57, lr 0.000468, consume 73.36s
step 27800, loss 3.56, lr 0.000467, consume 73.35s
step 27900, loss 3.57, lr 0.000466, consume 74.05s
step 28000, loss 3.56, lr 0.000465, consume 73.35s
step 28100, loss 3.56, lr 0.000464, consume 73.35s
step 28200, loss 3.57, lr 0.000463, consume 73.36s
step 28300, loss 3.56, lr 0.000462, consume 73.34s
step 28400, loss 3.56, lr 0.000461, consume 73.34s
processing 21: english_c4/c4-train.00021-of-01024.txt, origin: 355709, samples: 173358, accum_tokens: 3909M, iter_num: 28471
step 28500, loss 3.56, lr 0.000460, consume 92.68s
step 28600, loss 3.56, lr 0.000459, consume 75.61s
step 28700, loss 3.56, lr 0.000458, consume 94.90s
step 28800, loss 3.56, lr 0.000457, consume 73.52s
step 28900, loss 3.56, lr 0.000456, consume 73.66s
step 29000, loss 3.56, lr 0.000455, consume 73.66s
step 29100, loss 3.55, lr 0.000455, consume 73.65s
step 29200, loss 3.56, lr 0.000454, consume 73.64s
step 29300, loss 3.56, lr 0.000453, consume 73.65s
step 29400, loss 3.55, lr 0.000452, consume 73.65s
step 29500, loss 3.56, lr 0.000451, consume 73.64s
step 29600, loss 3.55, lr 0.000450, consume 74.22s
step 29700, loss 3.55, lr 0.000449, consume 73.64s
step 29800, loss 3.55, lr 0.000448, consume 73.65s
processing 22: english_c4/c4-train.00022-of-01024.txt, origin: 355742, samples: 173215, accum_tokens: 4086M, iter_num: 29825
step 29900, loss 3.56, lr 0.000447, consume 93.36s
step 30000, loss 3.57, lr 0.000446, consume 75.69s
step 30100, loss 3.56, lr 0.000445, consume 92.74s
step 30200, loss 3.56, lr 0.000444, consume 73.63s
step 30300, loss 3.55, lr 0.000443, consume 73.66s
step 30400, loss 3.56, lr 0.000442, consume 73.72s
step 30500, loss 3.56, lr 0.000441, consume 73.66s
step 30600, loss 3.55, lr 0.000440, consume 73.95s
step 30700, loss 3.55, lr 0.000439, consume 73.75s
step 30800, loss 3.55, lr 0.000438, consume 73.63s
step 30900, loss 3.56, lr 0.000437, consume 73.85s
step 31000, loss 3.55, lr 0.000436, consume 73.69s
step 31100, loss 3.55, lr 0.000435, consume 73.61s
processing 23: english_c4/c4-train.00023-of-01024.txt, origin: 355714, samples: 173979, accum_tokens: 4265M, iter_num: 31178
step 31200, loss 3.56, lr 0.000434, consume 92.97s
step 31300, loss 3.56, lr 0.000433, consume 75.86s
step 31400, loss 3.56, lr 0.000432, consume 94.82s
step 31500, loss 3.56, lr 0.000431, consume 73.48s
step 31600, loss 3.55, lr 0.000430, consume 73.61s
step 31700, loss 3.54, lr 0.000429, consume 73.61s
step 31800, loss 3.55, lr 0.000428, consume 73.59s
step 31900, loss 3.54, lr 0.000427, consume 74.28s
step 32000, loss 3.55, lr 0.000426, consume 73.61s
step 32100, loss 3.55, lr 0.000425, consume 73.61s
step 32200, loss 3.55, lr 0.000424, consume 73.60s
step 32300, loss 3.53, lr 0.000423, consume 73.61s
step 32400, loss 3.54, lr 0.000422, consume 73.41s
step 32500, loss 3.54, lr 0.000421, consume 73.36s
processing 24: english_c4/c4-train.00024-of-01024.txt, origin: 355728, samples: 172879, accum_tokens: 4442M, iter_num: 32537
step 32600, loss 3.54, lr 0.000420, consume 93.84s
step 32700, loss 3.55, lr 0.000419, consume 76.17s
step 32800, loss 3.54, lr 0.000418, consume 93.15s
step 32900, loss 3.55, lr 0.000417, consume 73.35s
step 33000, loss 3.55, lr 0.000416, consume 73.61s
step 33100, loss 3.55, lr 0.000414, consume 73.68s
step 33200, loss 3.55, lr 0.000413, consume 73.69s
step 33300, loss 3.54, lr 0.000412, consume 73.61s
step 33400, loss 3.54, lr 0.000411, consume 73.59s
step 33500, loss 3.55, lr 0.000410, consume 73.57s
step 33600, loss 3.54, lr 0.000409, consume 73.57s
step 33700, loss 3.54, lr 0.000408, consume 73.58s
step 33800, loss 3.54, lr 0.000407, consume 73.57s
processing 25: english_c4/c4-train.00025-of-01024.txt, origin: 355713, samples: 173638, accum_tokens: 4619M, iter_num: 33887
step 33900, loss 3.54, lr 0.000406, consume 92.72s
step 34000, loss 3.55, lr 0.000405, consume 75.00s
step 34100, loss 3.54, lr 0.000404, consume 93.97s
step 34200, loss 3.54, lr 0.000403, consume 73.43s
step 34300, loss 3.54, lr 0.000402, consume 73.64s
step 34400, loss 3.54, lr 0.000401, consume 73.66s
step 34500, loss 3.53, lr 0.000400, consume 73.66s
step 34600, loss 3.53, lr 0.000399, consume 73.65s
step 34700, loss 3.53, lr 0.000398, consume 73.66s
step 34800, loss 3.53, lr 0.000397, consume 73.72s
step 34900, loss 3.52, lr 0.000396, consume 73.66s
step 35000, loss 3.53, lr 0.000395, consume 73.66s
step 35100, loss 3.52, lr 0.000394, consume 73.65s
step 35200, loss 3.53, lr 0.000393, consume 73.66s
processing 26: english_c4/c4-train.00026-of-01024.txt, origin: 355688, samples: 173072, accum_tokens: 4797M, iter_num: 35244
step 35300, loss 3.53, lr 0.000391, consume 93.05s
step 35400, loss 3.53, lr 0.000390, consume 76.26s
step 35500, loss 3.53, lr 0.000389, consume 93.20s
step 35600, loss 3.54, lr 0.000388, consume 73.67s
step 35700, loss 3.53, lr 0.000387, consume 73.66s
step 35800, loss 3.53, lr 0.000386, consume 73.46s
step 35900, loss 3.53, lr 0.000385, consume 73.46s
step 36000, loss 3.52, lr 0.000384, consume 73.46s
step 36100, loss 3.53, lr 0.000383, consume 73.44s
step 36200, loss 3.53, lr 0.000382, consume 73.42s
step 36300, loss 3.53, lr 0.000381, consume 73.41s
step 36400, loss 3.52, lr 0.000380, consume 73.42s
step 36500, loss 3.53, lr 0.000379, consume 73.42s
processing 27: english_c4/c4-train.00027-of-01024.txt, origin: 355713, samples: 174471, accum_tokens: 4975M, iter_num: 36596
step 36600, loss 3.53, lr 0.000378, consume 90.05s
step 36700, loss 3.53, lr 0.000377, consume 76.97s
step 36800, loss 3.53, lr 0.000375, consume 95.31s
step 36900, loss 3.53, lr 0.000374, consume 73.16s
step 37000, loss 3.53, lr 0.000373, consume 73.37s
step 37100, loss 3.53, lr 0.000372, consume 73.39s
step 37200, loss 3.53, lr 0.000371, consume 73.40s
step 37300, loss 3.52, lr 0.000370, consume 73.38s
step 37400, loss 3.52, lr 0.000369, consume 73.40s
step 37500, loss 3.51, lr 0.000368, consume 73.38s
step 37600, loss 3.52, lr 0.000367, consume 73.37s
step 37700, loss 3.53, lr 0.000366, consume 73.35s
step 37800, loss 3.52, lr 0.000365, consume 73.38s
step 37900, loss 3.51, lr 0.000364, consume 73.38s
processing 28: english_c4/c4-train.00028-of-01024.txt, origin: 355701, samples: 173576, accum_tokens: 5153M, iter_num: 37959
step 38000, loss 3.52, lr 0.000363, consume 93.63s
step 38100, loss 3.53, lr 0.000361, consume 75.75s
step 38200, loss 3.52, lr 0.000360, consume 92.86s
step 38300, loss 3.52, lr 0.000359, consume 73.30s
step 38400, loss 3.52, lr 0.000358, consume 73.37s
step 38500, loss 3.52, lr 0.000357, consume 73.38s
step 38600, loss 3.52, lr 0.000356, consume 73.36s
step 38700, loss 3.52, lr 0.000355, consume 73.36s
step 38800, loss 3.52, lr 0.000354, consume 73.36s
step 38900, loss 3.52, lr 0.000353, consume 73.35s
step 39000, loss 3.52, lr 0.000352, consume 73.32s
step 39100, loss 3.51, lr 0.000351, consume 73.34s
step 39200, loss 3.52, lr 0.000350, consume 73.35s
step 39300, loss 3.52, lr 0.000348, consume 73.34s
processing 29: english_c4/c4-train.00029-of-01024.txt, origin: 355707, samples: 173593, accum_tokens: 5331M, iter_num: 39315
step 39400, loss 3.52, lr 0.000347, consume 94.95s
step 39500, loss 3.51, lr 0.000346, consume 75.79s
step 39600, loss 3.51, lr 0.000345, consume 93.39s
step 39700, loss 3.51, lr 0.000344, consume 73.38s
step 39800, loss 3.51, lr 0.000343, consume 73.40s
step 39900, loss 3.51, lr 0.000342, consume 73.40s
step 40000, loss 3.51, lr 0.000341, consume 73.41s
step 40100, loss 3.52, lr 0.000340, consume 73.39s
step 40200, loss 3.51, lr 0.000339, consume 73.40s
step 40300, loss 3.51, lr 0.000338, consume 73.38s
step 40400, loss 3.50, lr 0.000337, consume 73.37s
step 40500, loss 3.51, lr 0.000335, consume 73.37s
step 40600, loss 3.51, lr 0.000334, consume 73.39s
processing 30: english_c4/c4-train.00030-of-01024.txt, origin: 355751, samples: 173202, accum_tokens: 5508M, iter_num: 40671
step 40700, loss 3.50, lr 0.000333, consume 92.43s
step 40800, loss 3.51, lr 0.000332, consume 74.85s
step 40900, loss 3.51, lr 0.000331, consume 94.91s
step 41000, loss 3.51, lr 0.000330, consume 73.40s
step 41100, loss 3.50, lr 0.000329, consume 73.57s
step 41200, loss 3.51, lr 0.000328, consume 73.60s
step 41300, loss 3.51, lr 0.000327, consume 73.60s
step 41400, loss 3.51, lr 0.000326, consume 73.62s
step 41500, loss 3.51, lr 0.000325, consume 73.59s
step 41600, loss 3.50, lr 0.000323, consume 73.57s
step 41700, loss 3.50, lr 0.000322, consume 73.57s
step 41800, loss 3.51, lr 0.000321, consume 73.58s
step 41900, loss 3.50, lr 0.000320, consume 73.57s
step 42000, loss 3.50, lr 0.000319, consume 73.56s
processing 31: english_c4/c4-train.00031-of-01024.txt, origin: 355742, samples: 173547, accum_tokens: 5686M, iter_num: 42024
step 42100, loss 3.51, lr 0.000318, consume 93.33s
step 42200, loss 3.51, lr 0.000317, consume 75.52s
step 42300, loss 3.50, lr 0.000316, consume 92.47s
step 42400, loss 3.50, lr 0.000315, consume 73.53s
step 42500, loss 3.50, lr 0.000314, consume 73.57s
step 42600, loss 3.51, lr 0.000313, consume 73.57s
step 42700, loss 3.50, lr 0.000312, consume 73.56s
step 42800, loss 3.50, lr 0.000310, consume 73.54s
step 42900, loss 3.50, lr 0.000309, consume 73.53s
step 43000, loss 3.50, lr 0.000308, consume 73.52s
step 43100, loss 3.50, lr 0.000307, consume 73.52s
step 43200, loss 3.50, lr 0.000306, consume 73.53s
step 43300, loss 3.50, lr 0.000305, consume 73.55s
processing 32: english_c4/c4-train.00032-of-01024.txt, origin: 355738, samples: 174581, accum_tokens: 5865M, iter_num: 43380
step 43400, loss 3.50, lr 0.000304, consume 92.90s
step 43500, loss 3.50, lr 0.000303, consume 75.51s
step 43600, loss 3.50, lr 0.000302, consume 94.36s
step 43700, loss 3.50, lr 0.000301, consume 73.45s
step 43800, loss 3.50, lr 0.000300, consume 73.54s
step 43900, loss 3.50, lr 0.000299, consume 73.54s
step 44000, loss 3.50, lr 0.000297, consume 73.57s
step 44100, loss 3.49, lr 0.000296, consume 73.54s
step 44200, loss 3.50, lr 0.000295, consume 73.54s
step 44300, loss 3.50, lr 0.000294, consume 73.55s
step 44400, loss 3.49, lr 0.000293, consume 73.51s
step 44500, loss 3.50, lr 0.000292, consume 73.51s
step 44600, loss 3.49, lr 0.000291, consume 73.53s
step 44700, loss 3.49, lr 0.000290, consume 73.54s
processing 33: english_c4/c4-train.00033-of-01024.txt, origin: 355671, samples: 174239, accum_tokens: 6043M, iter_num: 44743
step 44800, loss 3.49, lr 0.000289, consume 94.51s
step 44900, loss 3.50, lr 0.000288, consume 75.27s
step 45000, loss 3.50, lr 0.000287, consume 94.54s
step 45100, loss 3.50, lr 0.000286, consume 73.50s
step 45200, loss 3.49, lr 0.000285, consume 73.53s
step 45300, loss 3.49, lr 0.000283, consume 73.56s
step 45400, loss 3.50, lr 0.000282, consume 73.56s
step 45500, loss 3.49, lr 0.000281, consume 73.57s
step 45600, loss 3.49, lr 0.000280, consume 73.57s
step 45700, loss 3.49, lr 0.000279, consume 73.55s
step 45800, loss 3.49, lr 0.000278, consume 73.56s
step 45900, loss 3.49, lr 0.000277, consume 73.56s
step 46000, loss 3.49, lr 0.000276, consume 73.55s
step 46100, loss 3.49, lr 0.000275, consume 73.56s
processing 34: english_c4/c4-train.00034-of-01024.txt, origin: 355742, samples: 173538, accum_tokens: 6221M, iter_num: 46104
step 46200, loss 3.49, lr 0.000274, consume 93.48s
step 46300, loss 3.49, lr 0.000273, consume 94.87s
step 46400, loss 3.49, lr 0.000272, consume 73.41s
step 46500, loss 3.49, lr 0.000271, consume 73.56s
step 46600, loss 3.49, lr 0.000270, consume 73.57s
step 46700, loss 3.48, lr 0.000269, consume 73.55s
step 46800, loss 3.48, lr 0.000268, consume 73.56s
step 46900, loss 3.49, lr 0.000266, consume 73.53s
step 47000, loss 3.49, lr 0.000265, consume 73.50s
step 47100, loss 3.49, lr 0.000264, consume 73.52s
step 47200, loss 3.48, lr 0.000263, consume 73.51s
step 47300, loss 3.47, lr 0.000262, consume 73.51s
step 47400, loss 3.49, lr 0.000261, consume 73.53s
processing 35: english_c4/c4-train.00035-of-01024.txt, origin: 355715, samples: 173348, accum_tokens: 6398M, iter_num: 47460
step 47500, loss 3.48, lr 0.000260, consume 93.01s
step 47600, loss 3.49, lr 0.000259, consume 75.71s
step 47700, loss 3.49, lr 0.000258, consume 94.90s
step 47800, loss 3.49, lr 0.000257, consume 73.47s
step 47900, loss 3.48, lr 0.000256, consume 73.53s
step 48000, loss 3.48, lr 0.000255, consume 73.52s
step 48100, loss 3.48, lr 0.000254, consume 73.50s
step 48200, loss 3.48, lr 0.000253, consume 73.49s
step 48300, loss 3.49, lr 0.000252, consume 73.51s
step 48400, loss 3.49, lr 0.000251, consume 73.49s
step 48500, loss 3.48, lr 0.000250, consume 73.49s
step 48600, loss 3.48, lr 0.000249, consume 73.48s
step 48700, loss 3.48, lr 0.000248, consume 73.49s
step 48800, loss 3.48, lr 0.000247, consume 73.47s
processing 36: english_c4/c4-train.00036-of-01024.txt, origin: 355725, samples: 173573, accum_tokens: 6576M, iter_num: 48814
step 48900, loss 3.48, lr 0.000246, consume 93.50s
step 49000, loss 3.48, lr 0.000245, consume 75.87s
step 49100, loss 3.48, lr 0.000243, consume 92.96s
step 49200, loss 3.48, lr 0.000242, consume 73.45s
step 49300, loss 3.48, lr 0.000241, consume 73.51s
step 49400, loss 3.47, lr 0.000240, consume 73.53s
step 49500, loss 3.48, lr 0.000239, consume 73.53s
step 49600, loss 3.48, lr 0.000238, consume 73.52s
step 49700, loss 3.47, lr 0.000237, consume 73.51s
step 49800, loss 3.47, lr 0.000236, consume 73.74s
step 49900, loss 3.47, lr 0.000235, consume 73.51s
step 50000, loss 3.47, lr 0.000234, consume 73.51s
step 50100, loss 3.47, lr 0.000233, consume 73.51s
processing 37: english_c4/c4-train.00037-of-01024.txt, origin: 355760, samples: 174025, accum_tokens: 6754M, iter_num: 50170
step 50200, loss 3.47, lr 0.000232, consume 93.77s
step 50300, loss 3.48, lr 0.000231, consume 75.92s
step 50400, loss 3.48, lr 0.000230, consume 92.52s
step 50500, loss 3.47, lr 0.000229, consume 73.46s
step 50600, loss 3.48, lr 0.000228, consume 73.53s
step 50700, loss 3.48, lr 0.000227, consume 73.55s
step 50800, loss 3.47, lr 0.000226, consume 73.54s
step 50900, loss 3.47, lr 0.000225, consume 73.54s
step 51000, loss 3.48, lr 0.000224, consume 73.52s
step 51100, loss 3.47, lr 0.000223, consume 73.54s
step 51200, loss 3.47, lr 0.000222, consume 73.52s
step 51300, loss 3.47, lr 0.000221, consume 73.51s
step 51400, loss 3.47, lr 0.000220, consume 73.52s
step 51500, loss 3.47, lr 0.000219, consume 73.53s
processing 38: english_c4/c4-train.00038-of-01024.txt, origin: 355686, samples: 173172, accum_tokens: 6932M, iter_num: 51530
step 51600, loss 3.47, lr 0.000218, consume 94.76s
step 51700, loss 3.47, lr 0.000217, consume 75.72s
step 51800, loss 3.47, lr 0.000216, consume 93.16s
step 51900, loss 3.47, lr 0.000215, consume 73.51s
step 52000, loss 3.46, lr 0.000214, consume 73.53s
step 52100, loss 3.46, lr 0.000213, consume 73.54s
step 52200, loss 3.46, lr 0.000212, consume 73.53s
step 52300, loss 3.46, lr 0.000211, consume 73.53s
step 52400, loss 3.47, lr 0.000210, consume 73.51s
step 52500, loss 3.47, lr 0.000209, consume 73.52s
step 52600, loss 3.46, lr 0.000208, consume 73.50s
step 52700, loss 3.46, lr 0.000207, consume 73.51s
step 52800, loss 3.47, lr 0.000206, consume 73.51s
processing 39: english_c4/c4-train.00039-of-01024.txt, origin: 355723, samples: 174130, accum_tokens: 7110M, iter_num: 52883
step 52900, loss 3.46, lr 0.000205, consume 92.76s
step 53000, loss 3.47, lr 0.000205, consume 74.79s
step 53100, loss 3.47, lr 0.000204, consume 95.56s
step 53200, loss 3.47, lr 0.000203, consume 73.43s
step 53300, loss 3.47, lr 0.000202, consume 73.55s
step 53400, loss 3.46, lr 0.000201, consume 73.54s
step 53500, loss 3.46, lr 0.000200, consume 73.54s
step 53600, loss 3.46, lr 0.000199, consume 73.53s
step 53700, loss 3.47, lr 0.000198, consume 73.52s
step 53800, loss 3.46, lr 0.000197, consume 73.53s
step 53900, loss 3.47, lr 0.000196, consume 73.53s
step 54000, loss 3.46, lr 0.000195, consume 73.53s
step 54100, loss 3.46, lr 0.000194, consume 73.53s
step 54200, loss 3.47, lr 0.000193, consume 73.53s
processing 40: english_c4/c4-train.00040-of-01024.txt, origin: 355743, samples: 174205, accum_tokens: 7288M, iter_num: 54243
step 54300, loss 3.46, lr 0.000192, consume 92.88s
step 54400, loss 3.46, lr 0.000191, consume 76.17s
step 54500, loss 3.46, lr 0.000190, consume 92.74s
step 54600, loss 3.46, lr 0.000189, consume 73.50s
step 54700, loss 3.46, lr 0.000188, consume 73.55s
step 54800, loss 3.46, lr 0.000188, consume 73.56s
step 54900, loss 3.46, lr 0.000187, consume 73.56s
step 55000, loss 3.45, lr 0.000186, consume 73.54s
step 55100, loss 3.46, lr 0.000185, consume 73.54s
step 55200, loss 3.46, lr 0.000184, consume 73.55s
step 55300, loss 3.46, lr 0.000183, consume 73.52s
step 55400, loss 3.46, lr 0.000182, consume 73.54s
step 55500, loss 3.46, lr 0.000181, consume 73.54s
step 55600, loss 3.45, lr 0.000180, consume 73.53s
processing 41: english_c4/c4-train.00041-of-01024.txt, origin: 355732, samples: 173841, accum_tokens: 7466M, iter_num: 55604
step 55700, loss 3.46, lr 0.000179, consume 93.89s
step 55800, loss 3.46, lr 0.000178, consume 95.77s
step 55900, loss 3.45, lr 0.000178, consume 73.37s
step 56000, loss 3.46, lr 0.000177, consume 73.52s
step 56100, loss 3.45, lr 0.000176, consume 73.54s
step 56200, loss 3.46, lr 0.000175, consume 73.54s
step 56300, loss 3.46, lr 0.000174, consume 73.53s
step 56400, loss 3.46, lr 0.000173, consume 73.54s
step 56500, loss 3.46, lr 0.000172, consume 73.55s
step 56600, loss 3.45, lr 0.000171, consume 73.53s
step 56700, loss 3.46, lr 0.000170, consume 73.54s
step 56800, loss 3.45, lr 0.000170, consume 73.52s
step 56900, loss 3.45, lr 0.000169, consume 73.52s
processing 42: english_c4/c4-train.00042-of-01024.txt, origin: 355693, samples: 172927, accum_tokens: 7643M, iter_num: 56962
step 57000, loss 3.46, lr 0.000168, consume 93.80s
step 57100, loss 3.47, lr 0.000167, consume 75.18s
step 57200, loss 3.45, lr 0.000166, consume 94.14s
step 57300, loss 3.46, lr 0.000165, consume 73.49s
step 57400, loss 3.45, lr 0.000164, consume 73.54s
step 57500, loss 3.45, lr 0.000163, consume 73.55s
step 57600, loss 3.46, lr 0.000163, consume 73.54s
step 57700, loss 3.45, lr 0.000162, consume 73.54s
step 57800, loss 3.45, lr 0.000161, consume 73.53s
step 57900, loss 3.45, lr 0.000160, consume 73.53s
step 58000, loss 3.45, lr 0.000159, consume 73.53s
step 58100, loss 3.45, lr 0.000158, consume 73.50s
step 58200, loss 3.44, lr 0.000158, consume 73.49s
step 58300, loss 3.45, lr 0.000157, consume 73.52s
processing 43: english_c4/c4-train.00043-of-01024.txt, origin: 355728, samples: 172642, accum_tokens: 7820M, iter_num: 58312
step 58400, loss 3.45, lr 0.000156, consume 93.28s
step 58500, loss 3.45, lr 0.000155, consume 76.66s
step 58600, loss 3.45, lr 0.000154, consume 91.24s
step 58700, loss 3.45, lr 0.000153, consume 73.49s
step 58800, loss 3.45, lr 0.000153, consume 73.50s
step 58900, loss 3.45, lr 0.000152, consume 73.52s
step 59000, loss 3.45, lr 0.000151, consume 73.49s
step 59100, loss 3.45, lr 0.000150, consume 73.53s
step 59200, loss 3.44, lr 0.000149, consume 73.54s
step 59300, loss 3.44, lr 0.000149, consume 73.51s
step 59400, loss 3.44, lr 0.000148, consume 73.53s
step 59500, loss 3.43, lr 0.000147, consume 73.52s
step 59600, loss 3.45, lr 0.000146, consume 73.54s
processing 44: english_c4/c4-train.00044-of-01024.txt, origin: 355718, samples: 173521, accum_tokens: 7998M, iter_num: 59661
step 59700, loss 3.45, lr 0.000145, consume 92.82s
step 59800, loss 3.44, lr 0.000145, consume 75.53s
step 59900, loss 3.45, lr 0.000144, consume 94.25s
step 60000, loss 3.44, lr 0.000143, consume 73.47s
step 60100, loss 3.45, lr 0.000142, consume 73.52s
step 60200, loss 3.44, lr 0.000141, consume 73.52s
step 60300, loss 3.44, lr 0.000141, consume 73.52s
step 60400, loss 3.43, lr 0.000140, consume 73.51s
step 60500, loss 3.43, lr 0.000139, consume 73.51s
step 60600, loss 3.45, lr 0.000138, consume 73.52s
step 60700, loss 3.43, lr 0.000138, consume 73.51s
step 60800, loss 3.44, lr 0.000137, consume 73.51s
step 60900, loss 3.43, lr 0.000136, consume 73.50s
step 61000, loss 3.44, lr 0.000135, consume 73.50s
processing 45: english_c4/c4-train.00045-of-01024.txt, origin: 355708, samples: 173877, accum_tokens: 8176M, iter_num: 61017
step 61100, loss 3.44, lr 0.000135, consume 93.44s
step 61200, loss 3.44, lr 0.000134, consume 76.61s
step 61300, loss 3.44, lr 0.000133, consume 93.85s
step 61400, loss 3.44, lr 0.000132, consume 73.51s
step 61500, loss 3.44, lr 0.000132, consume 73.54s
step 61600, loss 3.44, lr 0.000131, consume 73.54s
step 61700, loss 3.44, lr 0.000130, consume 73.52s
step 61800, loss 3.44, lr 0.000129, consume 73.53s
step 61900, loss 3.43, lr 0.000129, consume 73.53s
step 62000, loss 3.43, lr 0.000128, consume 73.52s
step 62100, loss 3.43, lr 0.000127, consume 73.54s
step 62200, loss 3.44, lr 0.000126, consume 73.52s
step 62300, loss 3.43, lr 0.000126, consume 73.53s
processing 46: english_c4/c4-train.00046-of-01024.txt, origin: 355711, samples: 173463, accum_tokens: 8354M, iter_num: 62375
step 62400, loss 3.44, lr 0.000125, consume 93.70s
step 62500, loss 3.44, lr 0.000124, consume 74.55s
step 62600, loss 3.43, lr 0.000124, consume 95.90s
step 62700, loss 3.43, lr 0.000123, consume 73.40s
step 62800, loss 3.44, lr 0.000122, consume 73.55s
step 62900, loss 3.44, lr 0.000122, consume 73.54s
step 63000, loss 3.43, lr 0.000121, consume 73.52s
step 63100, loss 3.44, lr 0.000120, consume 73.52s
step 63200, loss 3.43, lr 0.000119, consume 73.49s
step 63300, loss 3.43, lr 0.000119, consume 73.51s
step 63400, loss 3.43, lr 0.000118, consume 73.49s
step 63500, loss 3.42, lr 0.000117, consume 73.50s
step 63600, loss 3.43, lr 0.000117, consume 73.50s
step 63700, loss 3.43, lr 0.000116, consume 73.50s
processing 47: english_c4/c4-train.00047-of-01024.txt, origin: 355731, samples: 172936, accum_tokens: 8531M, iter_num: 63730
step 63800, loss 3.43, lr 0.000115, consume 93.99s
step 63900, loss 3.43, lr 0.000115, consume 75.48s
step 64000, loss 3.43, lr 0.000114, consume 94.72s
step 64100, loss 3.42, lr 0.000114, consume 73.48s
step 64200, loss 3.43, lr 0.000113, consume 73.52s
step 64300, loss 3.43, lr 0.000112, consume 73.53s
step 64400, loss 3.42, lr 0.000112, consume 73.51s
step 64500, loss 3.42, lr 0.000111, consume 73.50s
step 64600, loss 3.43, lr 0.000110, consume 73.50s
step 64700, loss 3.43, lr 0.000110, consume 73.51s
step 64800, loss 3.42, lr 0.000109, consume 73.50s
step 64900, loss 3.44, lr 0.000108, consume 73.52s
step 65000, loss 3.43, lr 0.000108, consume 73.51s
processing 48: english_c4/c4-train.00048-of-01024.txt, origin: 355721, samples: 172684, accum_tokens: 8707M, iter_num: 65081
step 65100, loss 3.43, lr 0.000107, consume 92.77s
step 65200, loss 3.43, lr 0.000107, consume 74.47s
step 65300, loss 3.42, lr 0.000106, consume 93.69s
step 65400, loss 3.43, lr 0.000105, consume 73.44s
step 65500, loss 3.42, lr 0.000105, consume 73.54s
step 65600, loss 3.43, lr 0.000104, consume 73.53s
step 65700, loss 3.42, lr 0.000104, consume 73.53s
step 65800, loss 3.43, lr 0.000103, consume 73.53s
step 65900, loss 3.42, lr 0.000102, consume 73.53s
step 66000, loss 3.42, lr 0.000102, consume 73.51s
step 66100, loss 3.43, lr 0.000101, consume 73.50s
step 66200, loss 3.42, lr 0.000101, consume 73.78s
step 66300, loss 3.44, lr 0.000100, consume 73.38s
step 66400, loss 3.43, lr 0.000100, consume 73.28s
processing 49: english_c4/c4-train.00049-of-01024.txt, origin: 355716, samples: 173827, accum_tokens: 8885M, iter_num: 66430
step 66500, loss 3.42, lr 0.000099, consume 93.48s
step 66600, loss 3.42, lr 0.000098, consume 75.42s
step 66700, loss 3.42, lr 0.000098, consume 92.48s
step 66800, loss 3.42, lr 0.000097, consume 73.26s
step 66900, loss 3.43, lr 0.000097, consume 73.31s
step 67000, loss 3.41, lr 0.000096, consume 73.31s
step 67100, loss 3.42, lr 0.000096, consume 73.30s
step 67200, loss 3.42, lr 0.000095, consume 73.29s
step 67300, loss 3.42, lr 0.000095, consume 73.29s
step 67400, loss 3.42, lr 0.000094, consume 73.30s
step 67500, loss 3.41, lr 0.000094, consume 73.30s
step 67600, loss 3.42, lr 0.000093, consume 73.29s
step 67700, loss 3.41, lr 0.000092, consume 73.28s
processing 50: english_c4/c4-train.00050-of-01024.txt, origin: 355718, samples: 172909, accum_tokens: 9062M, iter_num: 67788
step 67800, loss 3.41, lr 0.000092, consume 92.42s
step 67900, loss 3.42, lr 0.000091, consume 74.45s
step 68000, loss 3.43, lr 0.000091, consume 95.45s
step 68100, loss 3.43, lr 0.000090, consume 73.16s
step 68200, loss 3.42, lr 0.000090, consume 73.28s
step 68300, loss 3.43, lr 0.000089, consume 73.29s
step 68400, loss 3.42, lr 0.000089, consume 73.29s
step 68500, loss 3.42, lr 0.000088, consume 73.28s
step 68600, loss 3.41, lr 0.000088, consume 73.27s
step 68700, loss 3.42, lr 0.000087, consume 73.27s
step 68800, loss 3.42, lr 0.000087, consume 73.26s
step 68900, loss 3.42, lr 0.000087, consume 73.27s
step 69000, loss 3.41, lr 0.000086, consume 73.27s
step 69100, loss 3.41, lr 0.000086, consume 73.27s
processing 51: english_c4/c4-train.00051-of-01024.txt, origin: 355672, samples: 173579, accum_tokens: 9240M, iter_num: 69139
step 69200, loss 3.41, lr 0.000085, consume 94.09s
step 69300, loss 3.42, lr 0.000085, consume 75.39s
step 69400, loss 3.42, lr 0.000084, consume 92.35s
step 69500, loss 3.42, lr 0.000084, consume 73.26s
step 69600, loss 3.41, lr 0.000083, consume 73.28s
step 69700, loss 3.42, lr 0.000083, consume 73.29s
step 69800, loss 3.42, lr 0.000082, consume 73.30s
step 69900, loss 3.41, lr 0.000082, consume 73.29s
step 70000, loss 3.41, lr 0.000082, consume 73.29s
step 70100, loss 3.42, lr 0.000081, consume 73.28s
step 70200, loss 3.41, lr 0.000081, consume 73.28s
step 70300, loss 3.41, lr 0.000080, consume 73.29s
step 70400, loss 3.41, lr 0.000080, consume 73.29s
processing 52: english_c4/c4-train.00052-of-01024.txt, origin: 355741, samples: 174645, accum_tokens: 9419M, iter_num: 70495
step 70500, loss 3.41, lr 0.000080, consume 90.10s
step 70600, loss 3.42, lr 0.000079, consume 77.29s
step 70700, loss 3.42, lr 0.000079, consume 95.55s
step 70800, loss 3.41, lr 0.000078, consume 73.14s
step 70900, loss 3.42, lr 0.000078, consume 73.28s
step 71000, loss 3.41, lr 0.000078, consume 73.30s
step 71100, loss 3.41, lr 0.000077, consume 73.31s
step 71200, loss 3.41, lr 0.000077, consume 73.29s
step 71300, loss 3.41, lr 0.000076, consume 73.28s
step 71400, loss 3.41, lr 0.000076, consume 73.28s
step 71500, loss 3.41, lr 0.000076, consume 73.30s
step 71600, loss 3.41, lr 0.000075, consume 73.30s
step 71700, loss 3.41, lr 0.000075, consume 73.29s
step 71800, loss 3.41, lr 0.000075, consume 73.30s
processing 53: english_c4/c4-train.00053-of-01024.txt, origin: 355705, samples: 174657, accum_tokens: 9598M, iter_num: 71859
step 71900, loss 3.41, lr 0.000074, consume 92.54s
step 72000, loss 3.41, lr 0.000074, consume 74.89s
step 72100, loss 3.41, lr 0.000074, consume 94.34s
step 72200, loss 3.41, lr 0.000073, consume 73.25s
step 72300, loss 3.41, lr 0.000073, consume 73.29s
step 72400, loss 3.41, lr 0.000073, consume 73.31s
step 72500, loss 3.41, lr 0.000072, consume 73.30s
step 72600, loss 3.41, lr 0.000072, consume 73.28s
step 72700, loss 3.41, lr 0.000072, consume 73.28s
step 72800, loss 3.40, lr 0.000071, consume 73.29s
step 72900, loss 3.41, lr 0.000071, consume 73.28s
step 73000, loss 3.41, lr 0.000071, consume 73.28s
step 73100, loss 3.40, lr 0.000070, consume 73.28s
step 73200, loss 3.40, lr 0.000070, consume 73.26s
processing 54: english_c4/c4-train.00054-of-01024.txt, origin: 355738, samples: 174513, accum_tokens: 9777M, iter_num: 73223
step 73300, loss 3.41, lr 0.000070, consume 93.26s
step 73400, loss 3.41, lr 0.000069, consume 75.27s
step 73500, loss 3.41, lr 0.000069, consume 92.29s
step 73600, loss 3.41, lr 0.000069, consume 73.27s
step 73700, loss 3.41, lr 0.000069, consume 73.29s
step 73800, loss 3.41, lr 0.000068, consume 73.30s
step 73900, loss 3.40, lr 0.000068, consume 73.27s
step 74000, loss 3.40, lr 0.000068, consume 73.30s
step 74100, loss 3.41, lr 0.000068, consume 73.29s
step 74200, loss 3.41, lr 0.000067, consume 73.27s
step 74300, loss 3.41, lr 0.000067, consume 73.27s
step 74400, loss 3.41, lr 0.000067, consume 73.27s
step 74500, loss 3.40, lr 0.000067, consume 73.25s
processing 55: english_c4/c4-train.00055-of-01024.txt, origin: 355762, samples: 173354, accum_tokens: 9954M, iter_num: 74587
step 74600, loss 3.41, lr 0.000066, consume 93.63s
step 74700, loss 3.42, lr 0.000066, consume 75.27s
step 74800, loss 3.42, lr 0.000066, consume 94.40s
step 74900, loss 3.41, lr 0.000066, consume 73.16s
step 75000, loss 3.41, lr 0.000065, consume 73.27s
step 75100, loss 3.41, lr 0.000065, consume 73.29s
step 75200, loss 3.41, lr 0.000065, consume 73.27s
step 75300, loss 3.40, lr 0.000065, consume 73.27s
step 75400, loss 3.41, lr 0.000065, consume 73.26s
step 75500, loss 3.41, lr 0.000064, consume 73.25s
step 75600, loss 3.41, lr 0.000064, consume 73.25s
step 75700, loss 3.41, lr 0.000064, consume 73.25s
step 75800, loss 3.40, lr 0.000064, consume 73.26s
step 75900, loss 3.41, lr 0.000064, consume 73.25s
processing 56: english_c4/c4-train.00056-of-01024.txt, origin: 355728, samples: 173929, accum_tokens: 10132M, iter_num: 75941
step 76000, loss 3.41, lr 0.000063, consume 94.16s
step 76100, loss 3.40, lr 0.000063, consume 75.19s
step 76200, loss 3.41, lr 0.000063, consume 94.57s
step 76300, loss 3.41, lr 0.000063, consume 73.25s
step 76400, loss 3.40, lr 0.000063, consume 73.28s
step 76500, loss 3.40, lr 0.000063, consume 73.27s
step 76600, loss 3.40, lr 0.000063, consume 73.26s
step 76700, loss 3.41, lr 0.000062, consume 73.29s
step 76800, loss 3.40, lr 0.000062, consume 73.29s
step 76900, loss 3.40, lr 0.000062, consume 73.28s
step 77000, loss 3.41, lr 0.000062, consume 73.26s
step 77100, loss 3.39, lr 0.000062, consume 73.26s
step 77200, loss 3.40, lr 0.000062, consume 73.25s
step 77300, loss 3.40, lr 0.000062, consume 73.25s
processing 57: english_c4/c4-train.00057-of-01024.txt, origin: 355709, samples: 174072, accum_tokens: 10310M, iter_num: 77300
step 77400, loss 3.41, lr 0.000061, consume 93.31s
step 77500, loss 3.40, lr 0.000061, consume 96.06s
step 77600, loss 3.40, lr 0.000061, consume 73.13s
step 77700, loss 3.40, lr 0.000061, consume 73.27s
step 77800, loss 3.40, lr 0.000061, consume 73.27s
step 77900, loss 3.41, lr 0.000061, consume 73.27s
step 78000, loss 3.40, lr 0.000061, consume 73.26s
step 78100, loss 3.40, lr 0.000061, consume 73.25s
step 78200, loss 3.41, lr 0.000061, consume 73.24s
step 78300, loss 3.41, lr 0.000061, consume 73.24s
step 78400, loss 3.40, lr 0.000061, consume 73.24s
step 78500, loss 3.40, lr 0.000060, consume 73.24s
step 78600, loss 3.40, lr 0.000060, consume 73.25s
processing 58: english_c4/c4-train.00058-of-01024.txt, origin: 355737, samples: 173163, accum_tokens: 10488M, iter_num: 78659
step 78700, loss 3.41, lr 0.000060, consume 92.48s
step 78800, loss 3.40, lr 0.000060, consume 75.07s
step 78900, loss 3.40, lr 0.000060, consume 94.48s
step 79000, loss 3.41, lr 0.000060, consume 73.20s
step 79100, loss 3.41, lr 0.000060, consume 73.25s
step 79200, loss 3.40, lr 0.000060, consume 73.24s
step 79300, loss 3.40, lr 0.000060, consume 73.24s
step 79400, loss 3.40, lr 0.000060, consume 73.22s
step 79500, loss 3.41, lr 0.000060, consume 73.22s
step 79600, loss 3.41, lr 0.000060, consume 73.24s
step 79700, loss 3.41, lr 0.000060, consume 73.23s
step 79800, loss 3.40, lr 0.000060, consume 73.24s
step 79900, loss 3.40, lr 0.000060, consume 73.24s
step 80000, loss 3.40, lr 0.000060, consume 73.23s