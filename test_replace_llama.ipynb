{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl2020/Retriever/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/llama-2-7b-hf\", device_map=\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/llama-2-7b-hf\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replace_llama import replace_llama\n",
    "from microxcaling.mx import finalize_mx_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl2020/Retriever/.venv/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/tl2020/Retriever/.venv/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog. Who said this sentence?\\nThe quick brown fox jumps over the lazy dog.\\nWho said this sentence?\\nThe quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a sentence\n",
    "prompt = \"The quick brown fox jumps over the lazy dog. Who said this sentence?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, do_sample=False)\n",
    "generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing q_proj with microxcaling\n",
      "replacing k_proj with microxcaling\n",
      "replacing v_proj with microxcaling\n",
      "replacing o_proj with microxcaling\n",
      "replacing gate_proj with microxcaling\n",
      "replacing up_proj with microxcaling\n",
      "replacing down_proj with microxcaling\n",
      "replacing lm_head with microxcaling\n"
     ]
    }
   ],
   "source": [
    "# Simple MX spec for MXFP6 weights+activations\n",
    "mx_specs = {\n",
    "    'w_elem_format': 'int8',\n",
    "    'a_elem_format': 'int8',\n",
    "    'block_size': 32,\n",
    "    'bfloat': 16,\n",
    "    'custom_cuda': True,\n",
    "    # For quantization-aware finetuning, do backward pass in FP32\n",
    "    'quantize_backprop': False,\n",
    "}\n",
    "mx_specs = finalize_mx_specs(mx_specs)\n",
    "model = replace_llama(model, mx_specs).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog. Who said this sentence?\\nThe quick brown fox jumps over the lazy dog.\\nWho said this sentence?\\nThe quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a sentence\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, do_sample=False)\n",
    "generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "        (up_proj): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "        (down_proj): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "microxcaling.mx.linear.Linear"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "type(model.model.layers[0].self_attn.k_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
